{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01309e27",
   "metadata": {},
   "source": [
    "\n",
    "# 🌐 Open‑Source LLM Landscape (2025)\n",
    "\n",
    "**Prompt Engineering — Comprehensive Colab Notebook**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "1. **Survey** the major open‑source language‑model families, sizes, and licenses.  \n",
    "2. **Load & run** a lightweight OSS model via 🤗 Transformers.  \n",
    "3. **Compare** performance on the Open‑LLM Leaderboard.  \n",
    "4. **Understand** common permissive vs. restricted licenses.  \n",
    "5. **Explore** quantization & `llama.cpp` for local inference.  \n",
    "6. **Preview** evaluation & fine‑tuning workflows (LoRA/QLoRA).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c542822",
   "metadata": {},
   "source": [
    "\n",
    "## ⏳ Table of Contents\n",
    "1. [Introduction](#intro)  \n",
    "2. [Setup & Dependencies](#setup)  \n",
    "3. [First Hands‑On Inference](#hands-on)  \n",
    "4. [Model Family Overview](#families)  \n",
    "5. [Licensing 101](#license)  \n",
    "6. [Quantization & `llama.cpp`](#quant)  \n",
    "7. [Evaluation Quick‑Start](#eval)  \n",
    "8. [Fine‑Tuning Preview](#finetune)  \n",
    "9. [Exercises](#ex)  \n",
    "10. [Further Reading](#read)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849ec07",
   "metadata": {},
   "source": [
    "\n",
    "<a id='intro'></a>\n",
    "## 1️⃣ Introduction — Why Open Source LLMs?\n",
    "\n",
    "Open‑source language models accelerated **research reproducibility**, **deployment flexibility**, and a **lower barrier to entry** for startups, hobbyists, and academia. While proprietary frontier models often dominate raw benchmark scores, OSS models:\n",
    "\n",
    "* enable full‑stack auditing & transparency  \n",
    "* foster community‑driven safety improvements  \n",
    "* allow on‑prem or air‑gapped deployments  \n",
    "* spur innovation through forks and fine‑tunes  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca117d13",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2️⃣ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↳ This installs lightweight baseline deps (GPU‑ready on Colab)\n",
    "!pip -q install --upgrade transformers accelerate sentencepiece bitsandbytes --progress-bar off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27465b",
   "metadata": {},
   "source": [
    "<a id='hands-on'></a>\n",
    "## 3️⃣ Hands‑On — Load and Chat with **TinyLlama‑1.1B‑Chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, textwrap\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def chat(prompt, max_new_tokens=128, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_new_tokens,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Explain in simple terms what makes open‑source language models important.\", 64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46eff0",
   "metadata": {},
   "source": [
    "> ⚠️ *Colab’s free tier may OOM for models >4 B parameters.*  \n",
    "Try switching to a T4/RTX‑A100 runtime, or use a GGUF quantized model via `llama.cpp` (see Section 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf7b82",
   "metadata": {},
   "source": [
    "<a id='families'></a>\n",
    "## 4️⃣ Model Family Overview — Sizes, Licenses, Use‑Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models = [\n",
    "    (\"Llama‑3‑8B‑Instruct\", \"Meta\", \"8 B\", \"LLAMA‑3 Community\", \"2025‑04\", \"General chat, coding\"),\n",
    "    (\"Mixtral‑8×22B\", \"Mistral AI\", \"176 B (MoE)\", \"Apache‑2.0\", \"2024‑12\", \"High‑performance chat\"),\n",
    "    (\"Qwen1.5‑7B‑Chat\", \"Alibaba (Qwen)\", \"7 B\", \"Qianwen License v2\", \"2025‑01\", \"Multilingual, instruction\"),\n",
    "    (\"Gemma‑7B‑It\", \"Google DeepMind\", \"7 B\", \"Gemma License\", \"2024‑02\", \"Research, fine‑tuning\"),\n",
    "    (\"Phi‑3‑mini‑4k‑instruct\", \"Microsoft\", \"3.8 B\", \"MIT\", \"2025‑05\", \"Reasoning, mobile\"),\n",
    "    (\"TinyLlama‑1.1B‑Chat\", \"TinyLlama\", \"1.1 B\", \"Apache‑2.0\", \"2024‑10\", \"Lightweight demos\")\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(models, columns=[\"Model\", \"Org\", \"Params\", \"License\", \"Release\", \"Notes\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c671f2",
   "metadata": {},
   "source": [
    "\n",
    "<a id='license'></a>\n",
    "## 5️⃣ Licensing 101 — Permissive vs. Restricted\n",
    "\n",
    "| License | Permissive? | Commercial? | Derivatives? |\n",
    "|---------|-------------|-------------|--------------|\n",
    "| **Apache‑2.0** | ✅ | ✅ | ✅ w/ notice |\n",
    "| **MIT** | ✅ | ✅ | ✅ w/ notice |\n",
    "| **LLAMA 3 Community** | ⚠️ up to 700 M MAU | Limited | ✅ but cannot compete |\n",
    "| **Qianwen v2** | ⚠️ | Case‑by‑case | Same license |\n",
    "| **Gemma License** | ⚠️ | Free research; conditional commercial | Attribution & policy compliance |\n",
    "\n",
    "> Always read the full text! Even “open” licenses may cap monthly active users or restrict model competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899d840",
   "metadata": {},
   "source": [
    "\n",
    "<a id='quant'></a>\n",
    "## 6️⃣ Quantization & `llama.cpp`\n",
    "\n",
    "`llama.cpp` lets you run GGUF‑quantized models (e.g., 4‑bit) on CPU‑only systems.\n",
    "\n",
    "```bash\n",
    "# ≈ 40‑second build on Colab\n",
    "apt-get -qq install -y build-essential cmake\n",
    "pip install --quiet llama-cpp-python\n",
    "\n",
    "# Download a 4‑bit TinyLlama GGUF (≈ 500 MB)\n",
    "wget -q https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-GGUF/resolve/main/tinyllama-1.1b-chat.q4_K_M.gguf -O tiny.gguf\n",
    "```\n",
    "\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"tiny.gguf\", n_ctx=2048)\n",
    "print(llm(\"Q: What is the capital of France?\\nA:\", max_tokens=20)[\"choices\"][0][\"text\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdde6d1",
   "metadata": {},
   "source": [
    "\n",
    "<a id='eval'></a>\n",
    "## 7️⃣ Evaluation Quick‑Start\n",
    "\n",
    "```bash\n",
    "pip -q install lm-eval==0.4.2\n",
    "lm-eval --model hf --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0 --tasks truthful_qa --batch_size 8\n",
    "```\n",
    "\n",
    "This runs the **TruthfulQA** benchmark and outputs an accuracy percentage that you can compare with the [Open‑LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d76ec",
   "metadata": {},
   "source": [
    "\n",
    "<a id='finetune'></a>\n",
    "## 8️⃣ Fine‑Tuning Preview — LoRA in ~60 Lines\n",
    "\n",
    "```python\n",
    "!pip -q install peft datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_cfg = LoraConfig(task_type=\"CAUSAL_LM\", r=8, lora_alpha=32, lora_dropout=0.05)\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train[:1%]\").map(\n",
    "    lambda x: {\"text\": x[\"quote\"]}\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tinyllama-lora\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, training_args, train_dataset=dataset)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "This fine‑tunes only **3 %** of TinyLlama’s parameters thanks to LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a4ea7",
   "metadata": {},
   "source": [
    "\n",
    "<a id='ex'></a>\n",
    "## 9️⃣ Exercises\n",
    "1. **Swap Models:** Replace TinyLlama with `phi-3-mini-4k-instruct` and re‑run Section 3.  \n",
    "2. **License Audit:** Using the overview table, mark which models you could ship in a SaaS with >1 M MAU.  \n",
    "3. **Evaluation Challenge:** Run **HellaSwag** on two models and compare scores.  \n",
    "4. **LoRA Sprint:** Fine‑tune Gemma‑2B‑It on 100 lines of your own chat data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f02ee3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='read'></a>\n",
    "## 🔗 Further Reading & Leaderboards\n",
    "* Mistral AI — “Cheaper, Better, Faster, Stronger” release blog  \n",
    "* Meta — LLAMA 3 license and paper  \n",
    "* Alibaba Cloud — Qwen 1.5 Research License  \n",
    "* Hugging Face **Open‑LLM Leaderboard**  \n",
    "* *“Sparse Mixture‑of‑Experts Are All You Need”* (Mistral 2024)  \n",
    "* *“QLoRA: Efficient Fine‑Tuning of Quantized LLMs”* (ICML 2024)  \n",
    "\n",
    "Happy prompting! 🚀\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
