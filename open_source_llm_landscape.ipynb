{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01309e27",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒ Openâ€‘Source LLM Landscape (2025)\n",
    "\n",
    "**Prompt Engineering â€” Comprehensive Colab Notebook**\n",
    "\n",
    "---\n",
    "\n",
    "### LearningÂ Objectives\n",
    "1. **Survey** the major openâ€‘source languageâ€‘model families, sizes, and licenses.  \n",
    "2. **Load & run** a lightweight OSS model via ğŸ¤—Â Transformers.  \n",
    "3. **Compare** performance on the Openâ€‘LLM Leaderboard.  \n",
    "4. **Understand** common permissive vs. restricted licenses.  \n",
    "5. **Explore** quantization & `llama.cpp` for local inference.  \n",
    "6. **Preview** evaluation & fineâ€‘tuning workflows (LoRA/QLoRA).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c542822",
   "metadata": {},
   "source": [
    "\n",
    "## â³ TableÂ ofÂ Contents\n",
    "1. [Introduction](#intro)  \n",
    "2. [SetupÂ &Â Dependencies](#setup)  \n",
    "3. [FirstÂ Handsâ€‘OnÂ Inference](#hands-on)  \n",
    "4. [ModelÂ FamilyÂ Overview](#families)  \n",
    "5. [LicensingÂ 101](#license)  \n",
    "6. [QuantizationÂ &Â `llama.cpp`](#quant)  \n",
    "7. [EvaluationÂ Quickâ€‘Start](#eval)  \n",
    "8. [Fineâ€‘TuningÂ Preview](#finetune)  \n",
    "9. [Exercises](#ex)  \n",
    "10. [FurtherÂ Reading](#read)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849ec07",
   "metadata": {},
   "source": [
    "\n",
    "<a id='intro'></a>\n",
    "## 1ï¸âƒ£Â Introduction â€” Why OpenÂ SourceÂ LLMs?\n",
    "\n",
    "Openâ€‘source language models accelerated **research reproducibility**, **deployment flexibility**, and a **lower barrier to entry** for startups, hobbyists, and academia. While proprietary frontier models often dominate raw benchmark scores, OSS models:\n",
    "\n",
    "* enable fullâ€‘stack auditing & transparency  \n",
    "* foster communityâ€‘driven safety improvements  \n",
    "* allow onâ€‘prem or airâ€‘gapped deployments  \n",
    "* spur innovation through forks and fineâ€‘tunes  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca117d13",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2ï¸âƒ£Â SetupÂ &Â Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â†³ This installs lightweight baseline deps (GPUâ€‘ready on Colab)\n",
    "!pip -q install --upgrade transformers accelerate sentencepiece bitsandbytes --progress-bar off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27465b",
   "metadata": {},
   "source": [
    "<a id='hands-on'></a>\n",
    "## 3ï¸âƒ£Â Handsâ€‘OnÂ â€”Â LoadÂ andÂ ChatÂ with **TinyLlamaâ€‘1.1Bâ€‘Chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, textwrap\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def chat(prompt, max_new_tokens=128, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_new_tokens,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Explain in simple terms what makes openâ€‘source language models important.\", 64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46eff0",
   "metadata": {},
   "source": [
    "> âš ï¸ *Colabâ€™s free tier may OOM for models >4â€¯B parameters.*  \n",
    "Try switching to a T4/RTXâ€‘A100 runtime, or use a GGUF quantized model via `llama.cpp` (see SectionÂ 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf7b82",
   "metadata": {},
   "source": [
    "<a id='families'></a>\n",
    "## 4ï¸âƒ£Â ModelÂ FamilyÂ Overview â€” Sizes, Licenses, Useâ€‘Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models = [\n",
    "    (\"Llamaâ€‘3â€‘8Bâ€‘Instruct\", \"Meta\", \"8â€¯B\", \"LLAMAâ€‘3 Community\", \"2025â€‘04\", \"General chat, coding\"),\n",
    "    (\"Mixtralâ€‘8Ã—22B\", \"MistralÂ AI\", \"176â€¯BÂ (MoE)\", \"Apacheâ€‘2.0\", \"2024â€‘12\", \"Highâ€‘performance chat\"),\n",
    "    (\"Qwen1.5â€‘7Bâ€‘Chat\", \"AlibabaÂ (Qwen)\", \"7â€¯B\", \"QianwenÂ LicenseÂ v2\", \"2025â€‘01\", \"Multilingual, instruction\"),\n",
    "    (\"Gemmaâ€‘7Bâ€‘It\", \"GoogleÂ DeepMind\", \"7â€¯B\", \"GemmaÂ License\", \"2024â€‘02\", \"Research, fineâ€‘tuning\"),\n",
    "    (\"Phiâ€‘3â€‘miniâ€‘4kâ€‘instruct\", \"Microsoft\", \"3.8â€¯B\", \"MIT\", \"2025â€‘05\", \"Reasoning, mobile\"),\n",
    "    (\"TinyLlamaâ€‘1.1Bâ€‘Chat\", \"TinyLlama\", \"1.1â€¯B\", \"Apacheâ€‘2.0\", \"2024â€‘10\", \"Lightweight demos\")\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(models, columns=[\"Model\", \"Org\", \"Params\", \"License\", \"Release\", \"Notes\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c671f2",
   "metadata": {},
   "source": [
    "\n",
    "<a id='license'></a>\n",
    "## 5ï¸âƒ£Â LicensingÂ 101 â€” PermissiveÂ vs. Restricted\n",
    "\n",
    "| License | Permissive? | Commercial? | Derivatives? |\n",
    "|---------|-------------|-------------|--------------|\n",
    "| **Apacheâ€‘2.0** | âœ… | âœ… | âœ… w/ notice |\n",
    "| **MIT** | âœ… | âœ… | âœ… w/ notice |\n",
    "| **LLAMAÂ 3Â Community** | âš ï¸ up to 700â€¯M MAU | Limited | âœ… but cannot compete |\n",
    "| **QianwenÂ v2** | âš ï¸ | Caseâ€‘byâ€‘case | Same license |\n",
    "| **GemmaÂ License** | âš ï¸ | Free research; conditional commercial | Attribution & policy compliance |\n",
    "\n",
    "> Always read the full text! Even â€œopenâ€ licenses may cap monthly active users or restrict model competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899d840",
   "metadata": {},
   "source": [
    "\n",
    "<a id='quant'></a>\n",
    "## 6ï¸âƒ£Â QuantizationÂ &Â `llama.cpp`\n",
    "\n",
    "`llama.cpp` lets you run GGUFâ€‘quantized models (e.g., 4â€‘bit) on CPUâ€‘only systems.\n",
    "\n",
    "```bash\n",
    "# â‰ˆÂ 40â€‘second build on Colab\n",
    "apt-get -qq install -y build-essential cmake\n",
    "pip install --quiet llama-cpp-python\n",
    "\n",
    "# Download a 4â€‘bit TinyLlama GGUF (â‰ˆâ€¯500â€¯MB)\n",
    "wget -q https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-GGUF/resolve/main/tinyllama-1.1b-chat.q4_K_M.gguf -O tiny.gguf\n",
    "```\n",
    "\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"tiny.gguf\", n_ctx=2048)\n",
    "print(llm(\"Q: What is the capital of France?\\nA:\", max_tokens=20)[\"choices\"][0][\"text\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdde6d1",
   "metadata": {},
   "source": [
    "\n",
    "<a id='eval'></a>\n",
    "## 7ï¸âƒ£Â EvaluationÂ Quickâ€‘Start\n",
    "\n",
    "```bash\n",
    "pip -q install lm-eval==0.4.2\n",
    "lm-eval --model hf --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0 --tasks truthful_qa --batch_size 8\n",
    "```\n",
    "\n",
    "This runs the **TruthfulQA** benchmark and outputs an accuracy percentage that you can compare with the [Openâ€‘LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d76ec",
   "metadata": {},
   "source": [
    "\n",
    "<a id='finetune'></a>\n",
    "## 8ï¸âƒ£Â Fineâ€‘TuningÂ Preview â€” LoRAÂ inÂ ~60Â Lines\n",
    "\n",
    "```python\n",
    "!pip -q install peft datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_cfg = LoraConfig(task_type=\"CAUSAL_LM\", r=8, lora_alpha=32, lora_dropout=0.05)\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train[:1%]\").map(\n",
    "    lambda x: {\"text\": x[\"quote\"]}\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tinyllama-lora\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, training_args, train_dataset=dataset)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "This fineâ€‘tunes only **3â€¯%** of TinyLlamaâ€™s parameters thanks to LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a4ea7",
   "metadata": {},
   "source": [
    "\n",
    "<a id='ex'></a>\n",
    "## 9ï¸âƒ£Â Exercises\n",
    "1. **SwapÂ Models:** Replace TinyLlama with `phi-3-mini-4k-instruct` and reâ€‘run SectionÂ 3.  \n",
    "2. **License Audit:** Using the overview table, mark which models you could ship in a SaaS with >1â€¯M MAU.  \n",
    "3. **Evaluation Challenge:** Run **HellaSwag** on two models and compare scores.  \n",
    "4. **LoRA Sprint:** Fineâ€‘tune Gemmaâ€‘2Bâ€‘It on 100 lines of your own chat data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f02ee3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='read'></a>\n",
    "## ğŸ”—Â FurtherÂ Reading & Leaderboards\n",
    "* MistralÂ AI â€” â€œCheaper, Better, Faster, Strongerâ€ release blog  \n",
    "* Meta â€” LLAMAâ€¯3 license and paper  \n",
    "* Alibaba Cloud â€” QwenÂ 1.5 Research License  \n",
    "* HuggingÂ Face **Openâ€‘LLM Leaderboard**  \n",
    "* *â€œSparse Mixtureâ€‘ofâ€‘Experts Are All You Needâ€* (MistralÂ 2024)  \n",
    "* *â€œQLoRA: Efficient Fineâ€‘Tuning of Quantized LLMsâ€* (ICMLÂ 2024)  \n",
    "\n",
    "Happy prompting! ğŸš€\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
