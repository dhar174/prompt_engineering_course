{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442d8d12",
   "metadata": {},
   "source": [
    "# Prompt Engineering Notebook: OpenAI‚ÄëBased Multimodal Prompting\n",
    "*Google Colab‚ÄëCompatible ‚Äî Author: ChatGPT (o3) ‚Äî Date: 2025-07-09*\n",
    "\n",
    "This notebook demonstrates how to craft image‚Äë and audio‚Äëaware prompts using the **OpenAI Python SDK** and GPT‚Äë4o‚Äôs native multimodal capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bca9a",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Configure the OpenAI SDK for multimodal requests.\n",
    "2. Send **image‚Äëconditioned** chat completions using `image_url` blocks.\n",
    "3. Combine **multiple images** and text in one prompt.\n",
    "4. Transcribe and summarize audio with `audio.transcriptions.create`.\n",
    "5. Sketch a **Realtime API** loop for low‚Äëlatency speech‚Äëto‚Äëspeech.\n",
    "6. Evaluate & debug multimodal outputs with citations and safety checks.\n",
    "\n",
    "*All examples follow the explain‚Äëdemo‚Äëexercise pattern for classroom use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634915d0",
   "metadata": {},
   "source": [
    "## 0¬†|¬†Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07314b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install openai pillow python-dotenv\n",
    "# ‚Üë The OpenAI package ‚â•1.14.0 includes Vision & Realtime helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9015ab48",
   "metadata": {},
   "source": [
    "### Add Your OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('üîë OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9732ac1",
   "metadata": {},
   "source": [
    "## 1¬†|¬†Quick‚ÄëStart: Question‚ÄëAnswering over an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/multimodal/images/puppy.jpeg\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [\n",
    "             {\"type\": \"text\", \"text\": \"Describe the image in one sentence.\"},\n",
    "             {\"type\": \"image_url\", \"image_url\": {\"url\": img_url}}\n",
    "         ]}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12525461",
   "metadata": {},
   "source": [
    "> **How it works:**\n",
    "A `chat.completions.create` request can embed an `image_url` object directly in the `content` list. The model then ‚Äúsees‚Äù that image when forming its response. (*Syntax from the OpenAI Python SDK README*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b78eda",
   "metadata": {},
   "source": [
    "## 2¬†|¬†Multi‚ÄëImage & Text Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2684c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two images and decide which animal looks happier\n",
    "img1 = \"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/multimodal/images/cat.jpeg\"\n",
    "img2 = \"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/multimodal/images/dog.jpeg\"\n",
    "\n",
    "system_msg = \"You are an expert pet behaviorist.\"\n",
    "question = \"Which animal seems happier and why? Reply in two short sentences.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": question},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": img1}},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": img2}}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a88b8",
   "metadata": {},
   "source": [
    "**Exercise¬†üñºÔ∏è:** Swap the images for any two pictures you upload to Colab; observe whether the justification matches the visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7087d8",
   "metadata": {},
   "source": [
    "## 3¬†|¬†Vision¬†+¬†Function¬†Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "def parse_bbox(result_text):\n",
    "    # toy extractor for numbers inside brackets\n",
    "    nums = re.findall(r\"\\d+\", result_text)\n",
    "    return list(map(int, nums))\n",
    "\n",
    "# Define a tool schema:\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"store_bbox\",\n",
    "            \"description\": \"Save bounding box coordinates (x1,y1,x2,y2)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bbox\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"integer\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bbox\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "img_tool_url = img_url  # reuse puppy image\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Locate the puppy's face. Return bbox.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": img_tool_url}}\n",
    "        ]}\n",
    "    ],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "call = resp.choices[0].message.tool_calls[0]\n",
    "args = json.loads(call.function.arguments)\n",
    "print(\"Parsed bbox:\", args[\"bbox\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3814585",
   "metadata": {},
   "source": [
    "## 4¬†|¬†Audio¬†‚Üí¬†Text¬†‚Üí¬†Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2d867",
   "metadata": {},
   "source": [
    "*Upload a short MP3/WAV (<25¬†MB) via the Colab sidebar.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"demo_audio.mp3\"  # update after upload\n",
    "\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(audio_file, \"rb\"),\n",
    "    response_format=\"text\"\n",
    ").text\n",
    "\n",
    "summary = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You summarize transcripts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize in 3 bullet points:\\n{transcription}\"}\n",
    "    ]\n",
    ")\n",
    "print(\"\\nTRANSCRIPT:\\n\", transcription[:200], \"...\")\n",
    "print(\"\\nSUMMARY:\\n\", summary.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19c1d6",
   "metadata": {},
   "source": [
    "## 5¬†|¬†Realtime Speech¬†‚Üî¬†Speech Skeleton (Advanced)\n",
    "Below is a **conceptual** loop for the Beta Realtime API (speech‚Äëto‚Äëspeech). A full demo requires WebSockets and a mic stream, beyond this notebook‚Äôs scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pseudo\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "session = client.beta.realtime.sessions.create(model=\"gpt-4o\", format=\"wav\")\n",
    "for chunk in microphone_stream():\n",
    "    client.beta.realtime.sessions.send_audio_chunk(session.id, chunk)\n",
    "    for event in client.beta.realtime.sessions.receive_events(session.id):\n",
    "        if event.type == \"output_audio_chunk\":\n",
    "            play_audio(event.data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d949fc",
   "metadata": {},
   "source": [
    "‚û°Ô∏è¬†See the [OpenAI Cookbook realtime example](https://github.com/openai/openai-cookbook) for a complete reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d3e9f",
   "metadata": {},
   "source": [
    "## 6¬†|¬†Evaluation & Safety Checklist\n",
    "- **Grounding**: Does the answer reference actual image/audio evidence?\n",
    "- **Hallucination**: Flag when the model guesses unseen details.\n",
    "- **Privacy**: Strip faces/PII from stored media.\n",
    "- **Bias**: Test outputs across demographics and accents.\n",
    "- **Rate limits**: Vision calls are compute‚Äëheavy; handle 429 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804503ac",
   "metadata": {},
   "source": [
    "## Assignment üéì\n",
    "Build a *multimodal diary assistant* that:\n",
    "1. Accepts a daily photo and voice memo.\n",
    "2. Generates an uplifting caption plus a 2‚Äësentence reflection.\n",
    "3. Saves transcripts and captions to a CSV.\n",
    "4. Implements one safety check (e.g., blur NSFW images or skip them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307f7b6",
   "metadata": {},
   "source": [
    "## Further Reading & Resources\n",
    "- OpenAI Python SDK README (Vision examples)‚ÄØÓàÄciteÓàÇturn10search0ÓàÅ\n",
    "- GPT‚Äë4o Vision function‚Äëcalling notebook‚ÄØÓàÄciteÓàÇturn6search7ÓàÅ\n",
    "- Azure/OpenAI Vision how‚Äëto guide‚ÄØÓàÄciteÓàÇturn6search5ÓàÅ\n",
    "- Realtime API docs (speech streaming)‚ÄØÓàÄciteÓàÇturn8search0ÓàÅ\n",
    "- `audio.transcriptions.create` source reference‚ÄØÓàÄciteÓàÇturn11search0ÓàÅ"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
