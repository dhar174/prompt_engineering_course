{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b5728e",
   "metadata": {},
   "source": [
    "# Soft Prompting & Prompt Tuning Demo\n",
    "In this notebook you will:\n",
    "1. Understand the theory behind **soft prompts** (embeddingâ€‘based learned prompts).\n",
    "2. Experiment with **Prompt Tuning** and **Prefix (Pâ€‘)Tuning v2** using the PEFT library.\n",
    "3. Visualize how learned prompt embeddings steer generation compared to handâ€‘written prompts.\n",
    "---\n",
    "**Pedagogical goal:** illustrate how *only a handful of virtual tokens* can adapt a frozen model to a new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86edf7b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Colab may already have many of these packages. We install/upgrade the minimum set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade transformers peft datasets accelerate bitsandbytes sentencepiece\n",
    "# Restart the runtime after the first run if PEFT was newly installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9af17",
   "metadata": {},
   "source": [
    "## 2. Load a base model and small dataset\n",
    "We will fineâ€‘tune *only* the prompt embeddings on **SSTâ€‘2** (binary sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e18997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = 'gpt2'\n",
    "dataset = load_dataset('sst2', split='train[:200]')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a771e41b",
   "metadata": {},
   "source": [
    "### Formatting helper\n",
    "Convert each SSTâ€‘2 example into the typical *instruction â†’ answer* pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d07340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(ex):\n",
    "    label = 'positive' if ex['label']==1 else 'negative'\n",
    "    return {'text': f\"Review: {ex['sentence']}\\nSentiment:\" , 'label': label}\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7481fa3",
   "metadata": {},
   "source": [
    "## 3. Configure PEFT Prompt Tuning\n",
    "We freeze the model and train **M=20** virtual prompt tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model\n",
    "peft_config = PromptTuningConfig(task_type='CAUSAL_LM', num_virtual_tokens=20)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d925a8f",
   "metadata": {},
   "source": [
    "### Training loop (tiny â€“ just to illustrate)\n",
    "We use ðŸ¤— *datasets* to stream batches; one epoch suffices for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH=4\n",
    "def collate(batch):\n",
    "    inputs = tokenizer([b['text'] for b in batch], return_tensors='pt', padding=True)\n",
    "    labels = tokenizer([b['label'] for b in batch], return_tensors='pt', padding=True)\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH, shuffle=True, collate_fn=collate)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "model.train()\n",
    "for step, batch in enumerate(loader):\n",
    "    optim.zero_grad()\n",
    "    batch = {k:v.to(model.device) for k,v in batch.items()}\n",
    "    loss = model(**batch).loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step%50==0:\n",
    "        print(f'Step {step}, loss {loss.item():.3f}')\n",
    "    if step==200: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32a98d",
   "metadata": {},
   "source": [
    "## 4. Qualitative comparison\n",
    "Let's compare zeroâ€‘shot vs promptâ€‘tuned generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(review):\n",
    "    prompt = f'Review: {review}\\nSentiment:'\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=3)\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "generate('An awesome, heartâ€‘warming movie')\n",
    "generate('A dull, predictable mess')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87089c",
   "metadata": {},
   "source": [
    "## 5. Visualise learned prompt embeddings (tâ€‘SNE)\n",
    "We project the 20 virtual token embeddings into 2â€‘D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47076b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeds = model.get_input_embeddings().weight[-20:].cpu().numpy()\n",
    "coords = TSNE(n_components=2, perplexity=5).fit_transform(embeds)\n",
    "plt.scatter(coords[:,0], coords[:,1])\n",
    "for i,(x,y) in enumerate(coords):\n",
    "    plt.text(x, y, str(i))\n",
    "plt.title('Virtual prompt token embedding space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8589119",
   "metadata": {},
   "source": [
    "### Takeâ€‘aways\n",
    "- **Soft prompts** act as differentiable *keys* steering the LM.\n",
    "- We adapted GPTâ€‘2 to binary sentiment using **<1%** the parameters.\n",
    "- Similar methods: **Prefixâ€‘Tuning**, **Pâ€‘Tuning v2**, **LoRAâ€‘Prompts**.\n",
    "\n",
    "*Play with dataset size, number of virtual tokens, and model family!*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
