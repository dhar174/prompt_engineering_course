{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01570a7b",
   "metadata": {},
   "source": [
    "# Tokenization Playground\n",
    "Explore how text is broken into tokens for different LLMs and how context\u2011window limits affect prompts.\n",
    "\n",
    "Run the setup cell below in **Google\u00a0Colab** (or any Jupyter that allows `pip`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install tiktoken transformers\n",
    "import tiktoken, textwrap\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_openai_tokens(text, model='gpt-4o-mini'):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    return tokens, len(tokens)\n",
    "\n",
    "def count_hf_tokens(text, model_name='gpt2'):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tok.encode(text)\n",
    "    return tokens, len(tokens)\n",
    "\n",
    "print('\u2705\u00a0Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9884",
   "metadata": {},
   "source": [
    "## \ud83d\udd22  Count tokens for any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775359c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens, n = count_openai_tokens(sample)\n",
    "tokens_hf, n_hf = count_hf_tokens(sample)\n",
    "print(sample, '\n\u2192', n, 'tiktoken:', tokens)\n",
    "print('HF GPT-2:', n_hf, 'tokens:', tokens_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68295aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u270f\ufe0f\u00a0TRY IT: Replace the text below and re-run\n",
    "your_text = \"Replace me with any paragraph \u2026\"\n",
    "model = 'gpt-4o-mini'\n",
    "tokens, n = count_openai_tokens(your_text, model)\n",
    "tokens_hf, n_hf = count_hf_tokens(your_text)\n",
    "print(f'{n} tokens for model {model}')\n",
    "print('HF GPT-2:', n_hf, 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2b149",
   "metadata": {},
   "source": [
    "## \ud83e\uddee  Visualize vs. context window\n",
    "Below we compare your text length to a model\u2018s max context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u23f3 Context Window Sizes\n",
    "Different models support varying amounts of tokens in a single prompt.\n\n",
    "| Model | Max tokens |\n",
    "| --- | --- |\n",
    "| GPT\u20114o | 128k |\n",
    "| GPT\u20113.5 Turbo | 16k |\n",
    "| Claude 3 Haiku | 200k |\n",
    "| Llama 3 70B | 8k |\n",
    "\n",
    "Both your prompt and the model's reply must fit within these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7740d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_of_window(n_tokens, window=128_000):\n",
    "    pct = (n_tokens / window) * 100\n",
    "    return min(pct, 100)\n",
    "\n",
    "# Example for GPT\u20114o (128k context)\n",
    "_, n = count_openai_tokens(your_text)\n",
    "print(f'{n} tokens is {percent_of_window(n, 128_000):.2f}% of a 128k window')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f9916",
   "metadata": {},
   "source": [
    "---\n",
    "### Further Exploration\n",
    "* Try `AutoTokenizer` with non\u2011English text.\n",
    "* Examine how sub\u2011words split: e.g., `'cats'` vs `'cat' + 's'`.\n",
    "* Investigate token costs in the OpenAI pricing page."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
