{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01570a7b",
   "metadata": {},
   "source": [
    "# Tokenization Playground\n",
    "Explore how text is broken into tokens for different LLMs and how context‚Äëwindow limits affect prompts.\n",
    "\n",
    "Run the setup cell below in **Google¬†Colab** (or any Jupyter that allows `pip`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install tiktoken transformers\n",
    "import tiktoken, textwrap\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_openai_tokens(text, model='gpt-4o-mini'):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    return tokens, len(tokens)\n",
    "\n",
    "def count_hf_tokens(text, model_name='gpt2'):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tok.encode(text)\n",
    "    return tokens, len(tokens)\n",
    "\n",
    "print('‚úÖ¬†Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9884",
   "metadata": {},
   "source": [
    "## üî¢  Count tokens for any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775359c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens, n = count_openai_tokens(sample)\n",
    "print(sample, '\\n‚Üí', n, 'tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68295aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è¬†TRY IT: Replace the text below and re‚Äërun\n",
    "your_text = \"Replace me with any paragraph ‚Ä¶\"\n",
    "model = 'gpt-4o-mini'\n",
    "tokens, n = count_openai_tokens(your_text, model)\n",
    "print(f\"{n} tokens for model {model}\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2b149",
   "metadata": {},
   "source": [
    "## üßÆ  Visualize vs. context window\n",
    "Below we compare your text length to a model‚Äòs max context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7740d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_of_window(n_tokens, window=128k):\n",
    "    pct = (n_tokens / window) * 100\n",
    "    return min(pct, 100)\n",
    "\n",
    "# Example for GPT‚Äë4o (128k context)\n",
    "_, n = count_openai_tokens(your_text)\n",
    "print(f'{n} tokens is {percent_of_window(n, 128_000):.2f}% of a 128k window')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f9916",
   "metadata": {},
   "source": [
    "---\n",
    "### Further Exploration\n",
    "* Try `AutoTokenizer` with non‚ÄëEnglish text.\n",
    "* Examine how sub‚Äëwords split: e.g., `'cats'` vs `'cat' + 's'`.\n",
    "* Investigate token costs in the OpenAI pricing page."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
