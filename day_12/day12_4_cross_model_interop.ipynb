{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b7640d",
   "metadata": {},
   "source": [
    "\n",
    "# Day 12 · Notebook 4 — Cross‑Model Interoperability  (Concept #100)\n",
    "\n",
    "We will run the *same* prompt on three models—`gpt-4o-mini`, `meta-llama-3-8b-instruct`, and `mistralai/Mixtral‑7B`—then post‑process to a common JSON schema.\n",
    "\n",
    "> **Why?** Moving workloads between providers avoids vendor lock‑in and enables graceful degradation when one API rate‑limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc989b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: This demo uses the openai and together.ai endpoints as examples only.\n",
    "import json, os, openai, requests\n",
    "\n",
    "prompt = \"Extract the top three key takeaways (max 12 words each) from the text: 'Prompt engineering boosts LLM reliability.' Return JSON list.\"\n",
    "\n",
    "def gpt_call():\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=64,\n",
    "    ).choices[0].message.content\n",
    "\n",
    "def llama_call():\n",
    "    # pseudo‑call for illustrative purposes\n",
    "    resp = requests.post(\n",
    "        \"https://api.together.xyz/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {os.getenv('TOGETHER_API_KEY')}\" },\n",
    "        json={\n",
    "            \"model\": \"meta-llama/Llama-3-8b-instruct\",\n",
    "            \"messages\": [{\"role\":\"user\",\"content\": prompt}],\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 64,\n",
    "        },\n",
    "    ).json()\n",
    "    return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Parse & normalise outputs\n",
    "def normalise(raw:str) -> list:\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        # Fallback: naive bullet parser\n",
    "        return [ln.strip(\"-• \") for ln in raw.splitlines() if ln.strip()]\n",
    "\n",
    "for name, fn in [(\"GPT‑4o\", gpt_call), (\"Llama‑3‑8B\", llama_call)]:\n",
    "    out = normalise(fn())\n",
    "    print(name, \"→\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcfad1e",
   "metadata": {},
   "source": [
    "\n",
    "**Cheat‑sheet:**  \n",
    "| Provider | Temperature≈ | Stop tokens |\n",
    "|----------|--------------|-------------|\n",
    "| OpenAI   | 0‑0.2 for determinism | `\\nEND` |\n",
    "| Together | 0‑0.15 | `</s>` |\n",
    "\n",
    "Adjust these to keep outputs structurally aligned.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day12_4_cross_model_interop.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
