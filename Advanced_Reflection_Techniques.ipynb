{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47856ab",
   "metadata": {},
   "source": [
    "# Advanced Reflection Techniques in Prompt Engineering\n",
    "### Prompt Engineering Course â€“ Day X\n",
    "*Instructor: Charles Niswander II*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5b95e",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Explain** what â€œreflectionâ€ means for LLMs and why it improves reliability.  \n",
    "2. **Compare** baseline prompting with several *advanced* reflection patterns (Selfâ€‘Critique, Reflexion, Criticâ€‘Assistant, etc.).  \n",
    "3. **Implement** a twoâ€‘pass selfâ€‘evaluation loop using the OpenAI API (or a local model).  \n",
    "4. **Design & run** multiâ€‘turn reflection pipelines that automatically retry or revise answers.  \n",
    "5. **Measure** quality gains with simple programmatic or LLMâ€‘based evaluators.  \n",
    "6. **Extend** the templates provided here to new tasks (summarisation, coding, reasoning).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a85e73",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1Â Â Why Reflection?\n",
    "\n",
    "LLMs often generate the *first* plausible answer that satisfies the probability distribution, but that answer isnâ€™t always correct or safe.  \n",
    "**Reflection** adds one or more deliberate *metacognitive* steps in which the model (or a helper agent) reâ€‘reads, critiques, and revises its own output.\n",
    "\n",
    "Typical benefits:\n",
    "\n",
    "* Higher factual accuracy & fewer â€œsillyâ€ mistakes  \n",
    "* Better chainâ€‘ofâ€‘thought transparency for debugging  \n",
    "* A natural hook for toolâ€‘calling (tests, linters, content policies)  \n",
    "* Teachable pattern for students: â€œWrite âœ Evaluate âœ Improveâ€  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88afdc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1Â Â Basic vs. Advanced Reflection\n",
    "\n",
    "| Level | Pattern | # Passes | Typical Prompt Stub |\n",
    "|-------|---------|---------|----------------------|\n",
    "| Basic | *â€œThink step by stepâ€* | 1 | â€œLetâ€™s think step by step.â€ |\n",
    "| Intermediate | *Selfâ€‘Critique* | 2 | â€œEvaluate the previous answer. List flaws. Rewrite improved answer.â€ |\n",
    "| Advanced | *Reflexion / RCR / ReACT+Validation* | 3â€‘5 | â€œReflect on your reasoning, score it, and if below threshold, revise & try again (maxÂ _n_Â loops).â€ |\n",
    "\n",
    "In this notebook we move quickly to the **advanced** end of that spectrum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â— Installation (Colab only â€“ skip if already installed)\n",
    "# Feel free to swap in your favourite local model wrapper.\n",
    "!pip -q install --upgrade openai==1.23.0  # or latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a96e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "#@markdown ---  \n",
    "#@markdown ### ğŸ”‘Â Set your OpenAIÂ API key  \n",
    "#@markdown Store it in `OPENAI_API_KEY` env var **or** paste below (it will be hidden).\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"sk-...\"  # Replace if not set\n",
    "assert openai_api_key and openai_api_key.startswith(\"sk-\"), \"Please provide a valid key.\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "print(\"âœ…Â API key set (length:\", len(openai_api_key), \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai, textwrap, time, random\n",
    "openai_client = openai.OpenAI()  # uses env var\n",
    "\n",
    "def ask_model(sys_prompt, user_prompt, model=\"gpt-3.5-turbo\", max_tokens=256, temp=0.7):\n",
    "    '''Single call convenience wrapper'''\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2a0bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2Â Â Baseline Generation (No Reflection)\n",
    "\n",
    "Weâ€™ll use a deliberately tricky reasoning task that *looks* easy but often trips up the model.\n",
    "\n",
    "> **Task:** â€œHow many prime numbers are between 1 and 100?â€\n",
    "\n",
    "(AnswerÂ =Â 25.)\n",
    "\n",
    "Letâ€™s prompt the model *without* any reflection and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_answer = ask_model(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"How many prime numbers are between 1 and 100? Respond with just the number.\"\n",
    ")\n",
    "print(\"Model says:\", baseline_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701206fc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3Â Â Twoâ€‘Pass Selfâ€‘Critique\n",
    "\n",
    "We now ask the model to **check** its own answer:\n",
    "\n",
    "1. Generate an initial answer & chainâ€‘ofâ€‘thought.  \n",
    "2. Prompt: â€œCritique the answer above. If incorrect, provide a corrected answer.â€  \n",
    "\n",
    "We'll wrap that in a helper function so you can reuse it for any task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a523dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_critique(task, model=\"gpt-3.5-turbo\"):\n",
    "    sys_prompt = \"You are a large language model boosting your own reasoning via selfâ€‘critique.\"\n",
    "    initial = ask_model(sys_prompt, task + \"\\n\\nPlease explain your reasoning step by step.\", model=model)\n",
    "    critique_prompt = f\"\"\"**Your previous answer:**\n",
    "\n",
    "{initial}\n",
    "\n",
    "---\n",
    "\n",
    "**Critique Instructions (system):**\n",
    "Evaluate your answer. List any mistakes. Then give a FINAL_ANSWER line.\n",
    "\n",
    "Return format:\n",
    "1. Critique: <bullet list of issues or 'None'>\n",
    "2. FINAL_ANSWER: <single line answer only>\"\"\"\n",
    "    improved = ask_model(sys_prompt, critique_prompt, model=model)\n",
    "    return initial, improved\n",
    "\n",
    "init, refined = self_critique(\"How many prime numbers are between 1 and 100?\")\n",
    "print(\"---- Initial ----\\n\", init[:500], \"\\n\")\n",
    "print(\"---- Refined ----\\n\", refined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14422d70",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4Â Â Reflexionâ€‘Style Loops\n",
    "\n",
    "[Reflexion](https://arxiv.org/abs/2303.11366) combines *memory* of past failures with iterative selfâ€‘evaluation.\n",
    "\n",
    "**Algorithm sketch**\n",
    "\n",
    "```\n",
    "for attempt in range(max_iters):\n",
    "    answer, score = model(query + memory)\n",
    "    if score >= threshold:\n",
    "        break\n",
    "    memory += model(\"Why was that wrong? Summarise mistake.\")\n",
    "```\n",
    "\n",
    "Below is a *minimal* threeâ€‘try loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reflexion_loop(task, max_iters=3, model=\"gpt-3.5-turbo\"):\n",
    "    memory = \"\"\n",
    "    for i in range(1, max_iters+1):\n",
    "        user = f\"\"\"{task}\n",
    "{memory}\n",
    "---\n",
    "\n",
    "Please answer the TASK above. Then rate your own answer on a scale 0â€‘1 ('reflect_score'). \n",
    "If reflect_score < 0.9, explain what was wrong in <=2 sentences. \n",
    "Return JSON with keys: answer, reflect_score, reflection.\"\"\"\n",
    "        reply = ask_model(\"You are an expert reasoning agent.\", user, model=model, temp=0)\n",
    "        try:\n",
    "            data = eval(reply)  # crude but works for simple JSONâ€‘ish output\n",
    "            score = float(data[\"reflect_score\"])\n",
    "        except Exception as e:\n",
    "            print(\"Parsing fail on try\", i, \":\", e, \"\\nRaw:\", reply)\n",
    "            score = 0\n",
    "            data = {\"answer\":\"\", \"reflection\":\"\"}\n",
    "        print(f\"Try {i}, score={score}\")\n",
    "        if score>=0.9:\n",
    "            return data[\"answer\"]\n",
    "        memory += \"\\nMistake summary: \" + data[\"reflection\"]\n",
    "    return data[\"answer\"]\n",
    "\n",
    "best = reflexion_loop(\"How many prime numbers are between 1 and 100?\")\n",
    "print(\"Best answer:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ab72d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5Â Â Criticâ€‘Assistant Pattern\n",
    "\n",
    "Another strategy is to **split roles**:\n",
    "\n",
    "1. *Assistant* writes a draft.  \n",
    "2. *Critic* reviews and lists issues.  \n",
    "3. *Assistant* revises.  \n",
    "\n",
    "We can implement this with two model calls using different system prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def critic_assistant(task, model=\"gpt-3.5-turbo\"):\n",
    "    draft = ask_model(\"You are a creative assistant.\", task + \"\\nWrite your answer with reasoning.\")\n",
    "    critique = ask_model(\"You are a ruthless critic.\", \n",
    "                         f\"Here is an answer:\\n\\n{draft}\\n\\nList up to 3 fatal flaws.\")\n",
    "    revised = ask_model(\"You are a careful assistant.\", \n",
    "                        f\"Original answer:\\n{draft}\\n\\nCritique:\\n{critique}\\n\\nRewrite a perfect answer.\")\n",
    "    return draft, critique, revised\n",
    "\n",
    "d,c,r = critic_assistant(\"Explain why the sky is blue in two paragraphs, at an 8thâ€‘grade level.\")\n",
    "print(\"---- Draft ----\\n\", d[:400], \"\\n\")\n",
    "print(\"---- Critique ----\\n\", c)\n",
    "print(\"---- Revised ----\\n\", r[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8d834",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6Â Â Evaluating Reflection Quality\n",
    "\n",
    "Simple tasks (e.g., numeric answer) can be autoâ€‘graded in Python.  \n",
    "For openâ€‘ended tasks we can *reâ€‘use a model* as evaluator.\n",
    "\n",
    "Below is a toy grader for the primeâ€‘numbers task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prime_between_1_100(text):\n",
    "    try:\n",
    "        n = int(re.findall(r'\\d+', text)[0])\n",
    "        return n==25\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Baseline correct?\", prime_between_1_100(baseline_answer))\n",
    "print(\"Selfâ€‘Critique correct?\", prime_between_1_100(refined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdb2c0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7Â Â ğŸ“Â Exercises\n",
    "\n",
    "1. **Custom Task:** Pick a *reasoning* prompt where the base model often fails (e.g., â€œWhat day of the week was 4Â JulyÂ 1776?â€).  \n",
    "   * Implement a Reflexion loop that stops when the evaluator (another model call) says â€œconfidence â‰¥Â 0.95.â€  \n",
    "\n",
    "2. **Peer Review:** Form pairs. One writes a *draft* answer, the other critiques it, then swap.  \n",
    "   * Compare results with automated Criticâ€‘Assistant.  \n",
    "\n",
    "3. **Metric Design:** Modify the `prime_between_1_100` grader to score *explanations* for correctness, fluency & honesty on a 0â€‘5 scale using GPT.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c7ee2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8Â Â Key Takeaways\n",
    "\n",
    "* Reflection adds a **metacognitive layer** that boosts reliability.  \n",
    "* Advanced loops (Reflexion, Criticâ€‘Assistant) outperform simple *â€œLetâ€™s think step by step.â€*  \n",
    "* Always pair reflection with **evaluation** â€“ either heuristic or modelâ€‘based â€“ to know when to stop looping.  \n",
    "* Templates in this notebook are **starting points**: adapt them to your domain tasks, cost limits, and latency constraints.  \n",
    "\n",
    "**Further Reading**\n",
    "\n",
    "* Shin etÂ al., *Reflexion: Language Agents with Verbal Reinforcement Learning* (2023)  \n",
    "* Zheng etÂ al., *Judgingâ€¯LLMâ€™s reasoning chain of thought is difficult* (2024)  \n",
    "* Yao etÂ al., *Treeâ€‘ofâ€‘Thought Deliberate Reasoning* (2023)  \n",
    "\n",
    "Happy prompting! ğŸ‰\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
