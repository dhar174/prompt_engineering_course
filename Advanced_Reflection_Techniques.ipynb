{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47856ab",
   "metadata": {},
   "source": [
    "# Advanced Reflection Techniques in Prompt Engineering\n",
    "### Prompt Engineering Course ‚Äì Day X\n",
    "*Instructor: Charles Niswander II*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5b95e",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Explain** what ‚Äúreflection‚Äù means for LLMs and why it improves reliability.  \n",
    "2. **Compare** baseline prompting with several *advanced* reflection patterns (Self‚ÄëCritique, Reflexion, Critic‚ÄëAssistant, etc.).  \n",
    "3. **Implement** a two‚Äëpass self‚Äëevaluation loop using the OpenAI API (or a local model).  \n",
    "4. **Design & run** multi‚Äëturn reflection pipelines that automatically retry or revise answers.  \n",
    "5. **Measure** quality gains with simple programmatic or LLM‚Äëbased evaluators.  \n",
    "6. **Extend** the templates provided here to new tasks (summarisation, coding, reasoning).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a85e73",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1¬†¬†Why Reflection?\n",
    "\n",
    "LLMs often generate the *first* plausible answer that satisfies the probability distribution, but that answer isn‚Äôt always correct or safe.  \n",
    "**Reflection** adds one or more deliberate *metacognitive* steps in which the model (or a helper agent) re‚Äëreads, critiques, and revises its own output.\n",
    "\n",
    "Typical benefits:\n",
    "\n",
    "* Higher factual accuracy & fewer ‚Äúsilly‚Äù mistakes  \n",
    "* Better chain‚Äëof‚Äëthought transparency for debugging  \n",
    "* A natural hook for tool‚Äëcalling (tests, linters, content policies)  \n",
    "* Teachable pattern for students: ‚ÄúWrite ‚ûú Evaluate ‚ûú Improve‚Äù  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88afdc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1¬†¬†Basic vs. Advanced Reflection\n",
    "\n",
    "| Level | Pattern | # Passes | Typical Prompt Stub |\n",
    "|-------|---------|---------|----------------------|\n",
    "| Basic | *‚ÄúThink step by step‚Äù* | 1 | ‚ÄúLet‚Äôs think step by step.‚Äù |\n",
    "| Intermediate | *Self‚ÄëCritique* | 2 | ‚ÄúEvaluate the previous answer. List flaws. Rewrite improved answer.‚Äù |\n",
    "| Advanced | *Reflexion / RCR / ReACT+Validation* | 3‚Äë5 | ‚ÄúReflect on your reasoning, score it, and if below threshold, revise & try again (max¬†_n_¬†loops).‚Äù |\n",
    "\n",
    "In this notebook we move quickly to the **advanced** end of that spectrum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ùó Installation (Colab only ‚Äì skip if already installed)\n",
    "# Feel free to swap in your favourite local model wrapper.\n",
    "!pip -q install --upgrade openai==1.23.0  # or latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a96e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "#@markdown ---  \n",
    "#@markdown ### üîë¬†Set your OpenAI¬†API key  \n",
    "#@markdown Store it in `OPENAI_API_KEY` env var **or** paste below (it will be hidden).\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"sk-...\"  # Replace if not set\n",
    "assert openai_api_key and openai_api_key.startswith(\"sk-\"), \"Please provide a valid key.\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "print(\"‚úÖ¬†API key set (length:\", len(openai_api_key), \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai, textwrap, time, random\n",
    "openai_client = openai.OpenAI()  # uses env var\n",
    "\n",
    "def ask_model(sys_prompt, user_prompt, model=\"gpt-3.5-turbo\", max_tokens=256, temp=0.7):\n",
    "    '''Single call convenience wrapper'''\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2a0bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2¬†¬†Baseline Generation (No Reflection)\n",
    "\n",
    "We‚Äôll use a deliberately tricky reasoning task that *looks* easy but often trips up the model.\n",
    "\n",
    "> **Task:** ‚ÄúHow many prime numbers are between 1 and 100?‚Äù\n",
    "\n",
    "(Answer¬†=¬†25.)\n",
    "\n",
    "Let‚Äôs prompt the model *without* any reflection and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_answer = ask_model(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"How many prime numbers are between 1 and 100? Respond with just the number.\"\n",
    ")\n",
    "print(\"Model says:\", baseline_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701206fc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3¬†¬†Two‚ÄëPass Self‚ÄëCritique\n",
    "\n",
    "We now ask the model to **check** its own answer:\n",
    "\n",
    "1. Generate an initial answer & chain‚Äëof‚Äëthought.  \n",
    "2. Prompt: ‚ÄúCritique the answer above. If incorrect, provide a corrected answer.‚Äù  \n",
    "\n",
    "We'll wrap that in a helper function so you can reuse it for any task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a523dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_critique(task, model=\"gpt-3.5-turbo\"):\n",
    "    sys_prompt = \"You are a large language model boosting your own reasoning via self‚Äëcritique.\"\n",
    "    initial = ask_model(sys_prompt, task + \"\\n\\nPlease explain your reasoning step by step.\", model=model)\n",
    "    critique_prompt = f\"\"\"**Your previous answer:**\n",
    "\n",
    "{initial}\n",
    "\n",
    "---\n",
    "\n",
    "**Critique Instructions (system):**\n",
    "Evaluate your answer. List any mistakes. Then give a FINAL_ANSWER line.\n",
    "\n",
    "Return format:\n",
    "1. Critique: <bullet list of issues or 'None'>\n",
    "2. FINAL_ANSWER: <single line answer only>\"\"\"\n",
    "    improved = ask_model(sys_prompt, critique_prompt, model=model)\n",
    "    return initial, improved\n",
    "\n",
    "init, refined = self_critique(\"How many prime numbers are between 1 and 100?\")\n",
    "print(\"---- Initial ----\\n\", init[:500], \"\\n\")\n",
    "print(\"---- Refined ----\\n\", refined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14422d70",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4¬†¬†Reflexion‚ÄëStyle Loops\n",
    "\n",
    "[Reflexion](https://arxiv.org/abs/2303.11366) combines *memory* of past failures with iterative self‚Äëevaluation.\n",
    "\n",
    "**Algorithm sketch**\n",
    "\n",
    "```\n",
    "for attempt in range(max_iters):\n",
    "    answer, score = model(query + memory)\n",
    "    if score >= threshold:\n",
    "        break\n",
    "    memory += model(\"Why was that wrong? Summarise mistake.\")\n",
    "```\n",
    "\n",
    "Below is a *minimal* three‚Äëtry loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reflexion_loop(task, max_iters=3, model=\"gpt-3.5-turbo\"):\n",
    "    memory = \"\"\n",
    "    for i in range(1, max_iters+1):\n",
    "        user = f\"\"\"{task}\n",
    "{memory}\n",
    "---\n",
    "\n",
    "Please answer the TASK above. Then rate your own answer on a scale 0‚Äë1 ('reflect_score'). \n",
    "If reflect_score < 0.9, explain what was wrong in <=2 sentences. \n",
    "Return JSON with keys: answer, reflect_score, reflection.\"\"\"\n",
    "        reply = ask_model(\"You are an expert reasoning agent.\", user, model=model, temp=0)\n",
    "        try:\n",
    "            data = eval(reply)  # crude but works for simple JSON‚Äëish output\n",
    "            score = float(data[\"reflect_score\"])\n",
    "        except Exception as e:\n",
    "            print(\"Parsing fail on try\", i, \":\", e, \"\\nRaw:\", reply)\n",
    "            score = 0\n",
    "            data = {\"answer\":\"\", \"reflection\":\"\"}\n",
    "        print(f\"Try {i}, score={score}\")\n",
    "        if score>=0.9:\n",
    "            return data[\"answer\"]\n",
    "        memory += \"\\nMistake summary: \" + data[\"reflection\"]\n",
    "    return data[\"answer\"]\n",
    "\n",
    "best = reflexion_loop(\"How many prime numbers are between 1 and 100?\")\n",
    "print(\"Best answer:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ab72d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5¬†¬†Critic‚ÄëAssistant Pattern\n",
    "\n",
    "Another strategy is to **split roles**:\n",
    "\n",
    "1. *Assistant* writes a draft.  \n",
    "2. *Critic* reviews and lists issues.  \n",
    "3. *Assistant* revises.  \n",
    "\n",
    "We can implement this with two model calls using different system prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def critic_assistant(task, model=\"gpt-3.5-turbo\"):\n",
    "    draft = ask_model(\"You are a creative assistant.\", task + \"\\nWrite your answer with reasoning.\")\n",
    "    critique = ask_model(\"You are a ruthless critic.\", \n",
    "                         f\"Here is an answer:\\n\\n{draft}\\n\\nList up to 3 fatal flaws.\")\n",
    "    revised = ask_model(\"You are a careful assistant.\", \n",
    "                        f\"Original answer:\\n{draft}\\n\\nCritique:\\n{critique}\\n\\nRewrite a perfect answer.\")\n",
    "    return draft, critique, revised\n",
    "\n",
    "d,c,r = critic_assistant(\"Explain why the sky is blue in two paragraphs, at an 8th‚Äëgrade level.\")\n",
    "print(\"---- Draft ----\\n\", d[:400], \"\\n\")\n",
    "print(\"---- Critique ----\\n\", c)\n",
    "print(\"---- Revised ----\\n\", r[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8d834",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6¬†¬†Evaluating Reflection Quality\n",
    "\n",
    "Simple tasks (e.g., numeric answer) can be auto‚Äëgraded in Python.  \n",
    "For open‚Äëended tasks we can *re‚Äëuse a model* as evaluator.\n",
    "\n",
    "Below is a toy grader for the prime‚Äënumbers task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prime_between_1_100(text):\n",
    "    try:\n",
    "        n = int(re.findall(r'\\d+', text)[0])\n",
    "        return n==25\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Baseline correct?\", prime_between_1_100(baseline_answer))\n",
    "print(\"Self‚ÄëCritique correct?\", prime_between_1_100(refined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdb2c0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 7¬†¬†üìù¬†Exercises\n",
    "\n",
    "1. **Custom Task:** Pick a *reasoning* prompt where the base model often fails (e.g., ‚ÄúWhat day of the week was 4¬†July¬†1776?‚Äù).  \n",
    "   * Implement a Reflexion loop that stops when the evaluator (another model call) says ‚Äúconfidence ‚â•¬†0.95.‚Äù  \n",
    "\n",
    "2. **Peer Review:** Form pairs. One writes a *draft* answer, the other critiques it, then swap.  \n",
    "   * Compare results with automated Critic‚ÄëAssistant.  \n",
    "\n",
    "3. **Metric Design:** Modify the `prime_between_1_100` grader to score *explanations* for correctness, fluency & honesty on a 0‚Äë5 scale using GPT.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c7ee2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 8¬†¬†Key Takeaways\n",
    "\n",
    "* Reflection adds a **metacognitive layer** that boosts reliability.  \n",
    "* Advanced loops (Reflexion, Critic‚ÄëAssistant) outperform simple *‚ÄúLet‚Äôs think step by step.‚Äù*  \n",
    "* Always pair reflection with **evaluation** ‚Äì either heuristic or model‚Äëbased ‚Äì to know when to stop looping.  \n",
    "* Templates in this notebook are **starting points**: adapt them to your domain tasks, cost limits, and latency constraints.  \n",
    "\n",
    "**Further Reading**\n",
    "\n",
    "* Shin et¬†al., *Reflexion: Language Agents with Verbal Reinforcement Learning* (2023)  \n",
    "* Zheng et¬†al., *Judging‚ÄØLLM‚Äôs reasoning chain of thought is difficult* (2024)  \n",
    "* Yao et¬†al., *Tree‚Äëof‚ÄëThought Deliberate Reasoning* (2023)  \n",
    "\n",
    "Happy prompting! üéâ\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
