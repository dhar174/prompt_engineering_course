{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390449b0",
   "metadata": {},
   "source": [
    "# Lab 1 ‚Äì Prompt Pipelines: Chaining & Orchestration\n",
    "Colab‚Äëcompatible notebook demonstrating how to build modular prompt pipelines using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7212f34",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Understand prompt chaining principles.\n",
    "2. Build a simple linear chain.\n",
    "3. Explore graph / DAG orchestration patterns.\n",
    "4. Sketch your own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5139a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries (‚âà60‚ÄØs)\n",
    "!pip -q install langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febca369",
   "metadata": {},
   "source": [
    "### 1.¬†Warm‚Äëup: Simple Echo Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "# ‚ö†Ô∏è Paste your OpenAI key for live demo; keep it private!\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"üîë OpenAI API Key: \")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "resp = llm([HumanMessage(content=\"Say hello to the class!\")])\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4f86f",
   "metadata": {},
   "source": [
    "### 2.¬†Building a Linear Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "template1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a data retriever.\"),\n",
    "    (\"human\", \"{topic}\")\n",
    "])\n",
    "template2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarizer.\"),\n",
    "    (\"human\", \"{retrieved}\")\n",
    "])\n",
    "\n",
    "chain1 = template1 | llm | StrOutputParser()\n",
    "chain2 = template2 | llm | StrOutputParser()\n",
    "\n",
    "pipeline = SimpleSequentialChain(chains=[chain1, chain2], input_variables=[\"topic\"])\n",
    "print(pipeline.invoke({\"topic\": \"Key take‚Äëaways of reinforcement learning\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1a878",
   "metadata": {},
   "source": [
    "### 3.¬†Graph‚ÄëBased Orchestration\n",
    "For complex branching workflows, use `langchain.graphs` or tools like Prefect/Airflow. Below is a schematic example; run if you have Prefect installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b81c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try Prefect DAG orchestration\n",
    "# !pip -q install prefect\n",
    "# import prefect\n",
    "# from prefect import flow, task\n",
    "\n",
    "# @task\n",
    "# def retrieve(topic):\n",
    "#     return chain1.invoke({\"topic\": topic})\n",
    "\n",
    "# @task\n",
    "# def summarize(text):\n",
    "#     return chain2.invoke({\"retrieved\": text})\n",
    "\n",
    "# @flow\n",
    "# def pipeline(topic):\n",
    "#     raw = retrieve(topic)\n",
    "#     summ = summarize(raw)\n",
    "#     print(summ)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     pipeline(\"Vector databases and RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd2dc6",
   "metadata": {},
   "source": [
    "### 4.¬†Exercise ‚Äì Sketch Your Own Pipeline\n",
    "Use the cell below to outline a multi‚Äëstep prompt workflow relevant to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚¨áÔ∏è Your pipeline sketch here\n",
    "pipeline_steps = [\n",
    "    # (\"Step name\", \"Description\"),\n",
    "]\n",
    "for name, desc in pipeline_steps:\n",
    "    print(f\"{name}: {desc}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
