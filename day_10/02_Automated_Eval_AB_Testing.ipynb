{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97b4c0",
   "metadata": {},
   "source": [
    "# Notebook 2 – Automated Evaluation & A/B Testing\n",
    "Goals:\n",
    "1. Run controlled A/B prompt experiments\n",
    "2. Log results with **PromptLayer** (or fallback logging)\n",
    "3. Integrate with **OpenAI Evals** style harness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f027d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install promptlayer openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f5ccf",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Make sure you have set the environment variable `OPENAI_API_KEY` before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, json, uuid, time\n",
    "from promptlayer import promptlayer\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', 'sk-...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea672f",
   "metadata": {},
   "source": [
    "### Helper: `run_prompt(prompt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, model='gpt-3.5-turbo', temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{'role':'user','content': prompt}],\n",
    "        temperature=temperature)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9557e4",
   "metadata": {},
   "source": [
    "## 2. A/B Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11483705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test(prompt_a, prompt_b, n=5):\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        out_a = run_prompt(prompt_a)\n",
    "        out_b = run_prompt(prompt_b)\n",
    "        results.append({'trial': i, 'A': out_a, 'B': out_b})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25fbe6",
   "metadata": {},
   "source": [
    "Run your own prompts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e176c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'Summarise the following text in one sentence: ${input}'\n",
    "prompt2 = 'Provide a concise one‑sentence summary: ${input}'\n",
    "test_results = ab_test(prompt1, prompt2)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed2865",
   "metadata": {},
   "source": [
    "## 3. Quick Metric – ROUGE on A/B Outputs\n",
    "Replace `$reference` below with ground‑truth summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcee6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "reference = 'Your reference summary here.'\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "for row in test_results:\n",
    "    score_a = scorer.score(reference, row['A'])['rougeL'].fmeasure\n",
    "    score_b = scorer.score(reference, row['B'])['rougeL'].fmeasure\n",
    "    row['rougeL_A'] = score_a\n",
    "    row['rougeL_B'] = score_b\n",
    "test_results"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
