{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa57a68",
   "metadata": {},
   "source": [
    "# Notebook 1 – Core Evaluation Metrics\n",
    "In this notebook we will:\n",
    "* Install necessary evaluation libraries\n",
    "* Compute classic metrics (Accuracy, Precision, Recall, F1)\n",
    "* Evaluate generation quality with **BLEU** and **ROUGE**\n",
    "* Explore specialised benchmarks (TruthfulQA, Hallucination‑Rate)\n",
    "* Design a **human‑evaluation rubric** and capture results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install evaluate rouge-score datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aabec5",
   "metadata": {},
   "source": [
    "## 1. Classification Metrics Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "y_true = ['positive', 'negative', 'negative', 'positive']\n",
    "y_pred = ['positive', 'negative', 'positive', 'positive']\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', pos_label='positive')\n",
    "print(f'Accuracy: {acc:.2f}\\nPrecision: {prec:.2f}\\nRecall: {rec:.2f}\\nF1: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf58d0",
   "metadata": {},
   "source": [
    "## 2. Generation Metrics Demo – ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "reference = 'The cat sat on the mat.'\n",
    "candidate = 'A cat was sitting on the mat.'\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, candidate)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67808778",
   "metadata": {},
   "source": [
    "## 3. TruthfulQA Mini‑Check\n",
    "We will load a tiny subset from the *truthful_qa* dataset and score whether the model stays truthful. (For demonstration only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('truthful_qa', 'generation', split='validation[:5]')\n",
    "ds.to_pandas()[['question', 'best_answer']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ded1f7",
   "metadata": {},
   "source": [
    "## 4. Designing a Human‑Evaluation Rubric\n",
    "Fill in the table below with your criteria (1–5):\n",
    "\n",
    "| Criterion | 1 (Poor) | 3 (OK) | 5 (Great) |\n",
    "|-----------|----------|--------|-----------|\n",
    "| Relevance |          |        |           |\n",
    "| Factuality|          |        |           |\n",
    "| Style     |          |        |           |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
