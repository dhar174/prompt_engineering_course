{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b65e80a",
   "metadata": {},
   "source": [
    "# üå≥üï∏Ô∏è Tree‚Äë & Graph‚Äëof‚ÄëThought Explorer\n",
    "\n",
    "**Prompt Engineering ‚Äì Day¬†5 Advanced Reasoning Lab**\n",
    "\n",
    "Dive deep into *structured* multi‚Äëpath reasoning:\n",
    "\n",
    "* **Tree‚Äëof‚ÄëThoughts (ToT)** ‚Äì branch, evaluate, backtrack  \n",
    "* **Graph‚Äëof‚ÄëThoughts (GoT)** ‚Äì flexible DAG exploration & visualization  \n",
    "\n",
    "This Colab‚Äëready notebook combines conceptual primers, runnable algorithms, LLM‚Äëpowered search stubs (plug‚Äëin your key), and interactive exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7587fb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìë Learning goals\n",
    "\n",
    "1. Understand the difference between linear Chain‚Äëof‚ÄëThought and branching **Tree/Graph‚Äëof‚ÄëThought** paradigms.  \n",
    "2. Implement a generic **ToT search loop** with pluggable scoring functions (heuristic or LLM).  \n",
    "3. Visualize reasoning trajectories with **NetworkX** and **matplotlib**.  \n",
    "4. Experiment with beam width, depth limits, and reflection‚Äëbased pruning.  \n",
    "5. Apply ToT/GoT to toy puzzles (word scramble, arithmetic planning, path‚Äëfinding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c8d73",
   "metadata": {},
   "source": [
    "## üîß 0. Setup\n",
    "\n",
    "Install deps & set your OpenAI‚Äëcompatible key (optional ‚Äì heuristics will be used if absent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c604475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install --upgrade openai networkx matplotlib tiktoken\n",
    "import os, openai, random, math, itertools, collections, json, time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # leave blank to use heuristics only\n",
    "MODEL = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934d57b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ From Lines to Trees to Graphs\n",
    "\n",
    "* **Chain‚Äëof‚ÄëThought (CoT)**: single linear rationale  \n",
    "* **Tree‚Äëof‚ÄëThought (ToT)**: explore *multiple* partial reasoning states, keeping top‚Äëk at each depth  \n",
    "* **Graph‚Äëof‚ÄëThought (GoT)**: allow merges / loops ‚Äì states become nodes, transitions become edges\n",
    "\n",
    "Why? ‚Üí Better global search, ability to backtrack, parallel exploration, reduced hallucination via cross‚Äëchecking paths.\n",
    "\n",
    "![ToT vs GoT](https://i.imgur.com/7n5bX2e.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4013383",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Generic ToT Search Framework\n",
    "\n",
    "We‚Äôll build a reusable `TreeOfThoughtSolver`:\n",
    "\n",
    "```text\n",
    "while depth < max_depth:\n",
    "    expand top‚Äëk states ‚Üí children\n",
    "    score children (heuristic or LLM)\n",
    "    keep best beam_width states\n",
    "    optional: reflection/pruning\n",
    "```\n",
    "\n",
    "States and expansion logic are **task‚Äëspecific** ‚Äì we‚Äôll provide two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb493ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeOfThoughtSolver:\n",
    "    def __init__(self, expand_fn, score_fn, beam_width=5, max_depth=5, verbose=True):\n",
    "        \"\"\"Generic beam/breadth search.\"\"\"\n",
    "        self.expand_fn = expand_fn\n",
    "        self.score_fn = score_fn\n",
    "        self.beam_width = beam_width\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def solve(self, init_state):\n",
    "        frontier = [(init_state, 0.0)]  # (state, score)\n",
    "        for depth in range(self.max_depth):\n",
    "            if self.verbose:\n",
    "                print(f\"\\nDepth {depth} ‚Äì frontier size {len(frontier)}\")\n",
    "            candidates = []\n",
    "            for state, _ in frontier:\n",
    "                children = self.expand_fn(state)\n",
    "                for c in children:\n",
    "                    score = self.score_fn(c)\n",
    "                    candidates.append((c, score))\n",
    "            # keep top beam_width\n",
    "            frontier = sorted(candidates, key=lambda x: -x[1])[:self.beam_width]\n",
    "            # check goal\n",
    "            for s, sc in frontier:\n",
    "                if isinstance(sc, tuple) and sc[1]:  # (score, is_goal)\n",
    "                    return s\n",
    "        return frontier[0][0]  # best effort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146e491",
   "metadata": {},
   "source": [
    "### üß© Example¬†A ‚Äì Word Scramble\n",
    "\n",
    "Goal: unscramble 5‚Äëletter word (like 'TRAIN' ‚Üí 'RIANT', 'RATIN', 'NAIRT', etc.) that exists in English word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, itertools, random, string\n",
    "WORDS = {w.strip().lower() for w in requests.get('https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt').text.splitlines() if len(w)==5}\n",
    "\n",
    "def expand_scramble(state):\n",
    "    prefix, remaining = state\n",
    "    if not remaining:\n",
    "        return []\n",
    "    children=[]\n",
    "    for i,ch in enumerate(remaining):\n",
    "        new_prefix = prefix+ch\n",
    "        new_remaining = remaining[:i]+remaining[i+1:]\n",
    "        children.append((new_prefix, new_remaining))\n",
    "    return children\n",
    "\n",
    "def score_scramble(state):\n",
    "    prefix, remaining = state\n",
    "    if not remaining and prefix.lower() in WORDS:\n",
    "        return (1.0, True)  # perfect & goal\n",
    "    return (len(prefix)/5, False)\n",
    "\n",
    "scramble = \"TRAIN\"\n",
    "init_state = (\"\", scramble)\n",
    "solver = TreeOfThoughtSolver(expand_scramble, score_scramble, beam_width=10, max_depth=5)\n",
    "result = solver.solve(init_state)\n",
    "print(\"Best result:\", result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c292d",
   "metadata": {},
   "source": [
    "#### üìù Try‚Äëit\n",
    "\n",
    "Change `scramble` to other 5‚Äëletter jumbles.  \n",
    "Play with `beam_width` and `max_depth`.  \n",
    "Why might beam search sometimes miss a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15109c9a",
   "metadata": {},
   "source": [
    "### ‚ûï Example¬†B ‚Äì Arithmetic Planning via LLM Scoring\n",
    "\n",
    "Task: reach target number using given digits and `+ ‚àí √ó √∑`.  \n",
    "We‚Äôll let the *LLM* judge how close an expression is to the target (quick demonstration ‚Äì not guaranteed optimal!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c279b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 42\n",
    "digits = [3,7,9]\n",
    "ops = ['+','-','*','/']\n",
    "\n",
    "def expand_arith(expr):\n",
    "    if len(expr.split())//2 >= len(digits)-1:\n",
    "        return []\n",
    "    next_digit = digits[len(expr.split())//2 + 1]\n",
    "    children=[]\n",
    "    for op in ops:\n",
    "        children.append(expr + f\" {op} {next_digit}\")\n",
    "    return children\n",
    "\n",
    "def llm_score(expr):\n",
    "    if not openai.api_key:\n",
    "        # fallback heuristic: inverse absolute error\n",
    "        try:\n",
    "            val = eval(expr)\n",
    "            return -abs(target-val)\n",
    "        except ZeroDivisionError:\n",
    "            return -999\n",
    "    prompt = f\"Evaluate how close the value of the expression '{expr}' is to {target}. Respond with a single float: negative absolute error (higher is better).\" \n",
    "    try:\n",
    "        reply = chat(\"You are a grading function.\", prompt, temperature=0)\n",
    "        return float(reply)\n",
    "    except:\n",
    "        return -999\n",
    "\n",
    "init_expr = str(digits[0])\n",
    "solver2 = TreeOfThoughtSolver(expand_arith, llm_score, beam_width=4, max_depth=3)\n",
    "best = solver2.solve(init_expr)\n",
    "print(\"Best expression:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a81ac0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Graph‚Äëof‚ÄëThoughts Visualizer\n",
    "\n",
    "Convert explored states into a directed graph to see *all* reasoning paths, branch merges, and dead‚Äëends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_graph(init_state, expand_fn, depth=3):\n",
    "    G=nx.DiGraph()\n",
    "    G.add_node(str(init_state))\n",
    "    frontier=[init_state]\n",
    "    for _ in range(depth):\n",
    "        new=[]\n",
    "        for st in frontier:\n",
    "            for ch in expand_fn(st):\n",
    "                G.add_edge(str(st), str(ch))\n",
    "                new.append(ch)\n",
    "        frontier=new\n",
    "    return G\n",
    "\n",
    "G = explore_graph(init_state, expand_scramble, depth=3)\n",
    "plt.figure(figsize=(10,6))\n",
    "nx.draw(G, with_labels=False, node_size=150, arrows=False)\n",
    "plt.title(\"Graph‚Äëof‚ÄëThought (partial scramble search)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f194fd7",
   "metadata": {},
   "source": [
    "#### üìù Exercise\n",
    "\n",
    "* Replace `expand_scramble` with your own expansion logic (e.g., arithmetic plan).  \n",
    "* Use `nx.drawing.nx_pydot.write_dot(G, 'graph.dot')` to export for GraphViz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be36f0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Reflection‚ÄëBased Pruning (Bonus)\n",
    "\n",
    "Many ToT papers add a **reflection** step: ask the LLM to assess partial states and prune unlikely branches early.  \n",
    "Implement a simple demo (stubbed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_and_score(state):\n",
    "    prefix, remaining = state\n",
    "    if not openai.api_key:\n",
    "        # cheap heuristic\n",
    "        return (len(prefix)/5, False)\n",
    "    prompt = f\"You are judging word prefixes. Does '{prefix}' look like a valid English word prefix? Reply 1 for good, 0 for bad.\"\n",
    "    ok = chat(\"Judge\", prompt, temperature=0)\n",
    "    good = float(ok.strip() or 0)\n",
    "    return (good, False)\n",
    "\n",
    "solver3 = TreeOfThoughtSolver(expand_scramble, reflect_and_score, beam_width=5, max_depth=5)\n",
    "best = solver3.solve(init_state)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d84f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Capstone Challenge\n",
    "\n",
    "1. Pick **any** problem domain (logic puzzle, Sudoku, route planning).  \n",
    "2. Define state representation + `expand_fn` + `score_fn` *(heuristic or LLM)*.  \n",
    "3. Use `TreeOfThoughtSolver` to search for a solution.  \n",
    "4. Build a **Graph‚Äëof‚ÄëThought** for your search and visualize.  \n",
    "5. Reflect: where did branching help vs. hinder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8ea40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Key Papers\n",
    "\n",
    "* Yao et¬†al., ‚ÄúTree‚Äëof‚ÄëThought: Deliberate reasoning via expansive tree search‚Äù, 2023  \n",
    "* Long‚ÄØ&‚ÄØBosch, ‚ÄúGraph‚Äëof‚ÄëThought: Solving Elaborate Problems via Structured Memory‚Äù, 2024  \n",
    "* Gao et¬†al., ‚ÄúPlan-and-Reflect: Leveraging Self‚ÄëCritique for Better ToT Search‚Äù, 2024"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
