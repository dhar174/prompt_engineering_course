{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb2d482",
   "metadata": {},
   "source": [
    "# CoT & Selfâ€‘Consistency Playground  ğŸ²ğŸ§   \n",
    "\n",
    "**Prompt Engineering â€“ DayÂ 5 Lab Notebook**  \n",
    "\n",
    "Explore classic & modern Chainâ€‘ofâ€‘Thought (CoT) techniques, selfâ€‘consistency decoding, Treeâ€‘/Graphâ€‘ofâ€‘Thoughts, ReAct, Planâ€‘thenâ€‘Act, reflection, and generateâ€‘validate workflows â€“ all in one Colabâ€‘ready notebook.\n",
    "\n",
    "> Set your `OPENAI_API_KEY` (or plug in any chatâ€‘completion endpoint) and run the cells.  \n",
    "> Each section includes explanations, live demos, and **ğŸ“Â Tryâ€‘itâ€‘yourself** exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fb2a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š What youâ€™ll learn & do\n",
    "\n",
    "| Section | Concept | Handsâ€‘on |\n",
    "|---------|---------|----------|\n",
    "| 1 | Classic & Zeroâ€‘Shot CoT | Compare accuracy with/without â€œLetâ€™s think step by step.â€ |\n",
    "| 2 | Fewâ€‘Shot CoT & **Selfâ€‘Consistency** | Generate multiple rationales and majorityâ€‘vote |\n",
    "| 3 | Contrastive CoT | Ask for alternative lines of reasoning |\n",
    "| 4 | **Treeâ€‘ofâ€‘Thoughts (ToT)** | Simple breadthâ€‘first solver for a wordâ€‘scramble puzzle |\n",
    "| 5 | **Graphâ€‘ofâ€‘Thoughts (GoT)** | Visualise reasoning paths with NetworkX |\n",
    "| 6 | **ReAct** & Planâ€‘thenâ€‘Act | Interleave â€œThought/Actionâ€ with a toy calculator tool |\n",
    "| 7 | Metaâ€‘Prompting & Reflexion | Model critiques its own answer, then retries |\n",
    "| 8 | Generateâ€‘Validate Loop | Twoâ€‘stage pipeline with automatic retries |\n",
    "| 9 | Capstone Challenge | Build and share a multiâ€‘step reasoning pipeline |\n",
    "\n",
    "Feel free to skip around or duplicate cells while experimenting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e741d",
   "metadata": {},
   "source": [
    "## ğŸ”§ 0. Setup\n",
    "\n",
    "Run the cell below to install/update required packages and configure your OpenAI (or compatible) key.  \n",
    "Replace `\"YOUR_API_KEY\"` with your real key or export it via an environment variable first.\n",
    "\n",
    "```python\n",
    "%pip -q install --upgrade openai tiktoken\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, textwrap, random, math, time\n",
    "from collections import Counter, defaultdict\n",
    "import openai\n",
    "\n",
    "# âš ï¸Â SET YOUR KEY!\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_API_KEY\"\n",
    "MODEL = \"gpt-4o-mini\"  # swap to any chat-completion model you have access to\n",
    "\n",
    "def chat(system_prompt, user_prompt, temperature=0.7, max_tokens=512, model=MODEL):\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": system_prompt },\n",
    "        { \"role\": \"user\", \"content\": user_prompt }\n",
    "    ]\n",
    "    resp = openai.ChatCompletion.create(model=model,\n",
    "                                        messages=messages,\n",
    "                                        temperature=temperature,\n",
    "                                        max_tokens=max_tokens)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ› ï¸ Helper utilities\n",
    "\n",
    "def self_consistency(prompt, n=5, temperature=0.8):\n",
    "    \"\"\"Run the same prompt n times and return full rationales & majority answer.\"\"\"\n",
    "    rationales = [chat(\"You are a thoughtful assistant.\", prompt,\n",
    "                       temperature=temperature) for _ in range(n)]\n",
    "\n",
    "    # Heuristic: assume final line (after last newline) is the answer\n",
    "    answers = [r.split('\\n')[-1].strip() for r in rationales]\n",
    "    votes = Counter(answers)\n",
    "    majority, _ = votes.most_common(1)[0]\n",
    "    return rationales, votes, majority\n",
    "\n",
    "def pretty_print_votes(votes):\n",
    "    for ans, cnt in votes.items():\n",
    "        print(f\"{ans:>10}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61df26a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Classic & Zeroâ€‘Shot Chainâ€‘ofâ€‘Thought\n",
    "\n",
    "**Prompt pattern:** â€œLetâ€™s think step by step.â€\n",
    "\n",
    "Below we tackle a math word problem in three ways:\n",
    "\n",
    "1. **No CoT** â€“ single short answer.  \n",
    "2. **Classic CoT** â€“ insert the magic words and let the model show its reasoning.  \n",
    "3. **Zeroâ€‘Shot CoT** â€“ no examples, only the instruction.\n",
    "\n",
    "ğŸ‘‰ *Run the cells and observe differences in rationality & correctness.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ef3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"If there are 3 red marbles and 2 blue marbles in a bag, and you draw\n",
    "two marbles without replacement, what is the probability that both are red?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- No CoT ---\")\n",
    "print(chat(\"You are a concise assistant.\", problem + \"\\nAnswer succinctly.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Classic CoT ---\")\n",
    "print(chat(\"You are a helpful assistant.\", \"Let's think step by step.\\n\\n\" + problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57070a",
   "metadata": {},
   "source": [
    "### ğŸ“ Tryâ€‘itâ€‘yourself\n",
    "\n",
    "Change `problem` above to any word problem (math, logic, trivia) and reâ€‘run.  \n",
    "Which variant gives you higherâ€‘quality answers? Why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6951b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Fewâ€‘Shot CoT **+ Selfâ€‘Consistency**\n",
    "\n",
    "Provide 1â€‘2 worked examples to *bootstrap* reasoning style, then sample multiple\n",
    "rationales at higher temperature and majorityâ€‘vote for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = \"\"\"Q: If 4 pens cost $1.20, how much do 10 pens cost?\n",
    "A: Let's think step by step.\n",
    "- First find cost per pen: $1.20 / 4 = $0.30\n",
    "- Then 10 pens cost 10 * $0.30 = $3.00\n",
    "Therefore, the answer is $3.00\n",
    "\n",
    "Q: {question}\n",
    "A: Let's think step by step.\"\"\"\n",
    "question = \"A rectangle has a length of 8â€¯cm and a width of 5â€¯cm. What is its area?\"\n",
    "prompt = few_shot.format(question=question)\n",
    "rats, votes, majority = self_consistency(prompt, n=6, temperature=0.9)\n",
    "print(\"Rationalesâ†’\") \n",
    "for i,r in enumerate(rats,1):\n",
    "    print(f\"\\n--- Sample {i} ---\\n{r[:500]}\")\n",
    "\n",
    "print(\"\\nVote counts:\")\n",
    "pretty_print_votes(votes)\n",
    "print(\"\\nMajority answer:\", majority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27ce70",
   "metadata": {},
   "source": [
    "### ğŸ“ Exercise: Parameter sweep\n",
    "\n",
    "* Try different `n` and `temperature` values.  \n",
    "* Do majority votes always match ground truth?  \n",
    "* What kinds of errors appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b9562",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Contrastive / Alternative Reasoning\n",
    "\n",
    "Ask the model *twice* â€“ once for a solution, once for a **different** line of reasoning â€“ and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc553cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"\"\"{q}\n",
    "\n",
    "Provide one possible solution. Then, **in a separate paragraph**, outline an alternative reasoning path that could also arrive at the same answer, even if the steps differ.\"\"\".format(q=problem)\n",
    "\n",
    "print(chat(\"You are a reasoning assistant.\", base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405e8aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Treeâ€‘ofâ€‘Thoughts (ToT) Miniâ€‘Solver ğŸŒ³\n",
    "\n",
    "Weâ€™ll tackle a simple **wordâ€‘scramble** puzzle by exploring a search tree of partial solutions.\n",
    "\n",
    "*State =* current permutation of letters  \n",
    "*Score =* whether the permutation is a valid English word.\n",
    "\n",
    "> This is a toy example to illustrate branching, evaluation, and backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, random, requests, string, json, math\n",
    "# Super tiny word list\n",
    "WORDS = {w.strip().lower() for w in requests.get('https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt').text.splitlines() if len(w)==5}\n",
    "\n",
    "def is_word(w): return w.lower() in WORDS\n",
    "\n",
    "scramble = \"TRAIN\"\n",
    "print(\"Scramble:\", scramble)\n",
    "\n",
    "def tree_search(letters, max_branch=5, depth=5):\n",
    "    frontier=[(\"\", letters)]\n",
    "    for d in range(depth):\n",
    "        new=[]\n",
    "        for prefix, remain in frontier:\n",
    "            # expand top-k branches randomly for demo\n",
    "            for choice in random.sample(remain, min(max_branch,len(remain))):\n",
    "                new_pref = prefix+choice\n",
    "                new_rem = remain.replace(choice, '', 1)\n",
    "                if is_word(new_pref+new_rem):\n",
    "                    return new_pref+new_rem\n",
    "                new.append((new_pref, new_rem))\n",
    "        frontier=new\n",
    "    return None\n",
    "\n",
    "solution = tree_search(scramble, depth=5)\n",
    "print(\"Found:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c23a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Graphâ€‘ofâ€‘Thoughts (GoT) Visual Explorer ğŸ•¸ï¸\n",
    "\n",
    "Below we build a tiny reasoning graph for dividing an integer into factors.  \n",
    "Feel free to skip if youâ€™re short on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7338e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "n = 60\n",
    "G = nx.DiGraph()\n",
    "G.add_node(n)\n",
    "front=[n]\n",
    "while front:\n",
    "    cur=front.pop()\n",
    "    for i in range(2,int(math.sqrt(cur))+1):\n",
    "        if cur%i==0:\n",
    "            a,b = i, cur//i\n",
    "            G.add_edge(cur,a); G.add_edge(cur,b)\n",
    "            if a>1: front.append(a)\n",
    "            if b>1: front.append(b)\n",
    "plt.figure(figsize=(6,4))\n",
    "nx.draw(G, with_labels=True, node_size=500, font_size=8)\n",
    "plt.title(\"Graphâ€‘ofâ€‘Factors (Reasoning Paths)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a68930e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ ReAct & Planâ€‘thenâ€‘Act ğŸ¤–\n",
    "\n",
    "Weâ€™ll hook the LLM to a *tiny* calculator â€œtoolâ€ via prompting.\n",
    "\n",
    "```\n",
    "Thought: I need to add 23 + 19.\n",
    "Action: calculator.add(23,19)\n",
    "Observation: 42\n",
    "Thought: I have my answer.\n",
    "Answer: 42\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tool that the model can call by text\n",
    "def tool_executor(command:str):\n",
    "    try:\n",
    "        cmd, args = command.split('(')\n",
    "        args = args.rstrip(')')\n",
    "        nums = list(map(float, args.split(',')))\n",
    "        if cmd=='add': return sum(nums)\n",
    "        if cmd=='mul': \n",
    "            out=1\n",
    "            for n in nums: out*=n\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "query = \"What is (13*7) + (18*2)?\"\n",
    "\n",
    "prompt = f\"\"\"You have access to a calculator tool with syntax calculator.add(a,b) or calculator.mul(a,b,...).\n",
    "Respond using the ReAct format (Thought / Action / Observation) until you reach 'Answer:'.\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "\n",
    "# naive single call\n",
    "response = chat(\"You are a ReAct agent. When you need to compute, write Action: ...\", prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81e705",
   "metadata": {},
   "source": [
    "*Manually* copy any `calculator.xxx(...)` line you see in model output into `tool_executor` below to\n",
    "simulate Observation feedback, then feed the updated context back to the model (2â€‘3 iterations).  \n",
    "\n",
    "ğŸ‘‰Â Can you automate this loop? (Extension exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d948e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Metaâ€‘Prompting & Reflexion ğŸ”\n",
    "\n",
    "Ask the model to **critique** its own answer and retry if confidence is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Translate the following English sentence to French: 'The quick brown fox jumps over the lazy dog.'\"\n",
    "\n",
    "initial = chat(\"You are a translation assistant.\", task)\n",
    "critique = chat(\"You are a critical reviewer.\", \n",
    "                f\"Evaluate this translation for accuracy and fluency. If imperfect, explain why and give corrections:\\n\\n{initial}\")\n",
    "print(\"Initial translation:\", initial)\n",
    "print(\"\\nCritique & suggestions:\\n\", critique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb3b7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Generateâ€‘Validate Loop â™»ï¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a92c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validate(task, passes=3):\n",
    "    for i in range(passes):\n",
    "        answer = chat(\"You are a reasoning assistant.\", task)\n",
    "        judge  = chat(\"You are an impartial validator.\", \n",
    "                      f\"Task: {task}\\n\\nProposed answer: {answer}\\n\\nIs this answer correct? Reply YES or NO and explain.\")\n",
    "        if \"YES\" in judge.upper():\n",
    "            print(f\"âœ… Pass {i+1}: answer accepted\\n{answer}\")\n",
    "            return answer\n",
    "        print(f\"âŒ Pass {i+1}: retryingâ€¦\\nReason: {judge}\\n\")\n",
    "    return None\n",
    "\n",
    "generate_validate(\"What is the capital of Australia?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f98573",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Capstone Playground ğŸš€\n",
    "\n",
    "Design a *multiâ€‘step* prompt pipeline (or small Python loop) that:\n",
    "\n",
    "1. Uses **CoT/ToT/GoT** for initial reasoning  \n",
    "2. Applies **Selfâ€‘Consistency** or **Generateâ€‘Validate** for robustness  \n",
    "3. Includes at least one **Reflection/Critique** pass\n",
    "\n",
    "Share with classmates & try to break each otherâ€™s pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07affb81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— Further Reading\n",
    "\n",
    "* Wei etÂ al., â€œChainâ€‘ofâ€‘Thought Prompting Elicits Reasoning in Large Language Modelsâ€, 2022  \n",
    "* Yao etÂ al., â€œTreeâ€‘ofâ€‘Thought: Deliberate reasoning via expansive tree searchâ€, 2023  \n",
    "* Shinn etÂ al., â€œReflexion: Language Agents with Verbal Reinforcement Learningâ€, 2023  \n",
    "* Zhao etÂ al., â€œSelfâ€‘Consistency Improves Chain of Thought Reasoningâ€, 2022  \n",
    "* **OpenAI cookbook** â€“ [github.com/openai/openai-cookbook](https://github.com/openai/openai-cookbook)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
