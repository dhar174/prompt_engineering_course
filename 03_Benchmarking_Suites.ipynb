{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cb4299",
   "metadata": {},
   "source": [
    "# Notebook 3 – Benchmarking with LM Evaluation Harness & HELM\n",
    "You will learn to:\n",
    "* Install and run [lm‑eval‑harness](https://github.com/EleutherAI/lm‑eval‑harness)\n",
    "* Configure tasks (e.g., TruthfulQA, MMLU subset)\n",
    "* Parse and compare scores across prompt variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install lm‑evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fda434",
   "metadata": {},
   "source": [
    "## 1. Quick Run: TruthfulQA Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c110040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm‑eval --model gpt2 --tasks truthfulqa_mc:10 --device cpu --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f05692",
   "metadata": {},
   "source": [
    "> **Exercise**: Swap `--model` or add `--num_fewshot 5` to see how prompts impact accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607c83c",
   "metadata": {},
   "source": [
    "## 2. Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a648e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "res = json.load(open('eval_results.json'))  # path from lm‑eval output\n",
    "pd.DataFrame(res['results']).T.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
