{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96233cc",
   "metadata": {},
   "source": [
    "# Contrastive Chainâ€‘ofâ€‘Thought (Contrastiveâ€‘CoT)\n",
    "**Prompt Engineering â€“ GoogleÂ Colab Notebook**\n",
    "\n",
    "This notebook introduces **Contrastive Chainâ€‘ofâ€‘Thought (Contrastiveâ€‘CoT)** prompting, a recent technique that improves reasoning accuracy by *contrasting* multiple candidate explanations and selecting the most plausible one.\n",
    "\n",
    "> *Goal: Help your students understand, implement, and experiment with Contrastiveâ€‘CoT in practice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31548b8",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of this lab you will be able to\n",
    "1. **Describe** the motivation behind Chainâ€‘ofâ€‘Thought (CoT) prompting and its limitations.  \n",
    "2. **Explain** the intuition of contrastive reasoning and how it extends CoT.  \n",
    "3. **Implement** a simple Contrastiveâ€‘CoT pipeline with the OpenAI API (or any LLM).  \n",
    "4. **Measure** performance differences between *zeroâ€‘shot*, *standard CoT*, *selfâ€‘consistency*, and *Contrastiveâ€‘CoT*.  \n",
    "5. **Experiment** with your own tasks and hyperâ€‘parameters (number of rationales, contrastive prompts, vote rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bffaab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Â Â Background: from CoT to Contrastiveâ€‘CoT\n",
    "* **Chainâ€‘ofâ€‘Thought (CoT)**: ask the model to *think stepâ€‘byâ€‘step* before giving the final answer.  \n",
    "* **Selfâ€‘Consistency**: sample *k* random CoTs, then majorityâ€‘vote the final answers (WangÂ etÂ al.,Â 2022).  \n",
    "* **Why not enough?** Models sometimes produce *plausible but wrong* rationales. Majority vote may still pick the wrong answer if misleading rationales dominate.\n",
    "\n",
    "### 1.1Â Â Key idea\n",
    "Generate *pairs* of rationales and ask the model to judge **which one is more convincing** for the same question. The winning rationale is kept; the loser is discarded. Repeating tournamentâ€‘style selection yields a single, higherâ€‘quality explanation â†’ higher answer accuracy.\n",
    "\n",
    "> This â€œexplanation fightingâ€ is inspired by contrastive learning: good explanations should beat bad ones when placed sideâ€‘byâ€‘side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31197dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Â Â Highâ€‘level pipeline\n",
    "\n",
    "1. **Generate candidate rationales**  \n",
    "   Use a *â€œLetâ€™s think stepÂ byÂ stepâ€* style prompt *n* times (temperatureÂ >Â 0) â†’ list of *(rationale, answer)* pairs.\n",
    "2. **Contrastive selection**  \n",
    "   Repeatedly sample *m* distinct candidates, ask the model:  \n",
    "   *â€œHere are two ways to solve the problem. Which answer is more likely correct and why?â€*  \n",
    "   Keep the winner of each duel. Iterate until one champion remains (or use a scoring function).\n",
    "3. **Return championâ€™s answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b59917",
   "metadata": {},
   "source": [
    "> ðŸ›  **Setup:** The next code cell installs/updates required libraries.  \n",
    "*Skip if already installed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247610d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title â¬‡ï¸ Install & import\n",
    "!pip -q install --upgrade openai tiktoken python-dotenv rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda56db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json, re\n",
    "from typing import Tuple, List\n",
    "import openai\n",
    "from rich import print\n",
    "\n",
    "# â¬‡ï¸ðŸ“ Set your API key as an environment variable or paste directly (not recommended)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', 'paste-your-key-here')\n",
    "\n",
    "MODEL_NAME = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11c0e3",
   "metadata": {},
   "source": [
    "### 3Â Â Helper: call the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(system: str, user: str, temperature: float = 0.7, max_tokens: int = 512):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            { 'role':'system', 'content': system },\n",
    "            { 'role':'user', 'content': user }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d61e1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Â Â MiniÂ evaluation set\n",
    "We will use a *tiny* synthetic maths & logic dataset for quick experimentation. Feel free to replace with BIGâ€‘BENCH or GSMâ€‘8K for homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        'question': 'Tom has 3 blue marbles and buys twice as many red marbles. How many marbles does he have now in total?',\n",
    "        'answer': '9'\n",
    "    },\n",
    "    {\n",
    "        'question': 'If today is Wednesday, what day of the week will it be in 19 days?',\n",
    "        'answer': 'Monday'\n",
    "    },\n",
    "    {\n",
    "        'question': 'A rectangle is twice as long as it is wide. If its perimeter is 36â€¯cm, what is its area?',\n",
    "        'answer': '80'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419dc06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Â Â Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_answer(q: str) -> str:\n",
    "    prompt = f\"{q}\\nAnswer:\"\n",
    "    return chat_completion('You are a helpful solver.', prompt, temperature=0.0, max_tokens=8)\n",
    "\n",
    "def cot_answer(q: str, temp=0.7) -> str:\n",
    "    prompt = f\"{q}\\nLet's think step by step.\"\n",
    "    full = chat_completion('You are an expert reasoning assistant.', prompt, temperature=temp)\n",
    "    m = re.search(r\"\\bAnswer\\s*[:=]?\\s*(?P<ans>[-+]?\\d+(?:\\.\\d+)?|[A-Za-z]+)\\b\", full)\n",
    "    return m.group('ans') if m else full.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85819291",
   "metadata": {},
   "source": [
    "### 5.1Â Â Evaluate helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict_fn):\n",
    "    correct = 0\n",
    "    for ex in dataset:\n",
    "        pred = predict_fn(ex['question'])\n",
    "        if str(pred).lower().strip() == str(ex['answer']).lower().strip():\n",
    "            correct += 1\n",
    "    return correct / len(dataset)\n",
    "\n",
    "print('Direct (noâ€‘CoT) accuracy:', accuracy(direct_answer))\n",
    "print('1â€‘sample CoT accuracy:', accuracy(cot_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0d2ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Â Â Selfâ€‘Consistency CoT (review)\n",
    "\n",
    "We sample *k* diverse CoTs and majorityâ€‘vote their answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency(q: str, k=5):\n",
    "    answers = [cot_answer(q, temp=1.0) for _ in range(k)]\n",
    "    best = max(set(answers), key=answers.count)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d335ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selfâ€‘Consistency accuracy (k=5):', accuracy(lambda q: self_consistency(q, k=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8aa71d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Â Â Contrastiveâ€‘CoT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6226e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duel(q: str, cand1: Tuple[str,str], cand2: Tuple[str,str]) -> Tuple[str,str]:\n",
    "    \"\"\"Ask the model to choose between two candidate explanations.\"\"\"\n",
    "    prompt = (\n",
    "        f\"Question: {q}\\n\\n\"\n",
    "        f\"Candidate A reasoning:\\n{cand1[0]}\\n\\n\"\n",
    "        f\"Candidate B reasoning:\\n{cand2[0]}\\n\\n\"\n",
    "        \"Between A and B, which answer is more likely correct? Reply with either 'A' or 'B' and briefly justify.\"\n",
    "    )\n",
    "    choice = chat_completion('You are a strict judge of explanations.', prompt, temperature=0.2, max_tokens=32)\n",
    "    winner = cand1 if choice.strip().upper().startswith('A') else cand2\n",
    "    return winner\n",
    "\n",
    "def contrastive_cot(q: str, n_generate: int = 6, rounds: int = 3) -> str:\n",
    "    \"\"\"Main pipeline.\"\"\"\n",
    "    # Step 1: generate candidates\n",
    "    candidates: List[Tuple[str,str]] = []\n",
    "    for _ in range(n_generate):\n",
    "        gen_prompt = (\n",
    "            f\"{q}\\n\\n\"\n",
    "            \"Let's think step by step. Give the final answer in a line that starts with 'Answer:'\"\n",
    "        )\n",
    "        reasoning = chat_completion('You are an expert problemâ€‘solver.', gen_prompt, temperature=1.0)\n",
    "        m = re.search(r\"Answer\\s*[:=]\\s*(?P<ans>[-+]?\\d+|[A-Za-z]+)\", reasoning)\n",
    "        ans = m.group('ans') if m else 'unknown'\n",
    "        candidates.append((reasoning, ans))\n",
    "\n",
    "    # Step 2: tournament duels\n",
    "    for _ in range(rounds):\n",
    "        if len(candidates) < 2:\n",
    "            break\n",
    "        random.shuffle(candidates)\n",
    "        winners: List[Tuple[str,str]] = []\n",
    "        for i in range(0, len(candidates) - 1, 2):\n",
    "            winners.append(duel(q, candidates[i], candidates[i+1]))\n",
    "        if len(candidates) % 2 == 1:\n",
    "            winners.append(candidates[-1])\n",
    "        candidates = winners\n",
    "\n",
    "    champion_reasoning, champion_ans = candidates[0]\n",
    "    return champion_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Contrastiveâ€‘CoT accuracy:', accuracy(lambda q: contrastive_cot(q, n_generate=6, rounds=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06156bb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Â Â Discussion\n",
    "\n",
    "* Contrastiveâ€‘CoT often **beats** plain CoT because the model is *better at judging than generating*.  \n",
    "* Computational cost â†‘ (extra API calls). Tradeâ€‘off between *n_generate* and accuracy.  \n",
    "* Judge bias: The same model serves as both generator and critic. Using a **different critic model** (e.g. `gpt-4o` as judge) can further help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc4ec6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Â Â Your turnÂ ðŸŽ“\n",
    "1. Try larger `n_generate` (e.g.Â 10) and more `rounds`.  \n",
    "2. Swap in your own tasks (GSMâ€‘8K JSON file).  \n",
    "3. Use a stronger critic model.  \n",
    "4. Plot accuracy vs. cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f572c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10Â Â References\n",
    "* YeÂ etâ€¯al., *Contrastive Decoding* (2023)  \n",
    "* ZhangÂ etâ€¯al., *Selfâ€‘Consistency Improves Chainâ€‘ofâ€‘Thought Reasoning* (2022)  \n",
    "* OpenAI Cookbook: <https://github.com/openai/openai-cookbook>\n",
    "\n",
    "*Notebook prepared for **Charles N.'s Prompt Engineering class** â€“ JulyÂ 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
