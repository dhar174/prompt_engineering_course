{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982538db",
   "metadata": {},
   "source": [
    "# PAL & Plan‑Then‑Act Pipeline\n",
    "LLM writes Python, we execute it, then ask LLM to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install openai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, ipywidgets as w, contextlib, io, traceback\n",
    "from IPython.display import display, Markdown\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'sk-')\n",
    "\n",
    "problem=w.Textarea(value='What is the factorial of 8?',description='Problem:',layout=w.Layout(width='100%',height='60px'))\n",
    "run_btn=w.Button(description='Run PAL')\n",
    "out=w.Output()\n",
    "\n",
    "PAL_SYS='''You are an assistant that solves problems by first planning then writing Python.\\nRespond with three fenced blocks plan/python/answer.''' \n",
    "\n",
    "def run(_):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        r=openai.ChatCompletion.create(model='gpt-4o-mini',\n",
    "            messages=[{'role':'system','content':PAL_SYS},{'role':'user','content':problem.value}],\n",
    "            temperature=0.3,max_tokens=400)\n",
    "        txt=r.choices[0].message.content\n",
    "        display(Markdown('### LLM Response\\n'+txt))\n",
    "        if '```python' not in txt:\n",
    "            print('No code block.');return\n",
    "        code=txt.split('```python')[1].split('```')[0]\n",
    "        buf=io.StringIO()\n",
    "        try:\n",
    "            with contextlib.redirect_stdout(buf):\n",
    "                exec(code, {})\n",
    "            out_str=buf.getvalue().strip()\n",
    "        except Exception:\n",
    "            out_str=traceback.format_exc()\n",
    "        print('--- Code Output ---');print(out_str)\n",
    "        exp=openai.ChatCompletion.create(model='gpt-4o-mini',\n",
    "            messages=[{'role':'system','content':'Explain the result.'},{'role':'user','content':out_str}],\n",
    "            temperature=0.2,max_tokens=150)\n",
    "        display(Markdown('### Explanation\\n'+exp.choices[0].message.content))\n",
    "run_btn.on_click(run)\n",
    "display(w.VBox([problem, run_btn, out]))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
