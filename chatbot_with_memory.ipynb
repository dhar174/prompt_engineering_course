{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9317,
     "status": "ok",
     "timestamp": 1738285618695,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "MRnOiLHlsylW",
    "outputId": "31cfbb8a-c34e-4c96-841a-9288ec65b47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.68-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.33)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.10-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.2.68-py3-none-any.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.0.10-py3-none-any.whl (37 kB)\n",
      "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
      "Successfully installed langgraph-0.2.68 langgraph-checkpoint-2.0.10 langgraph-sdk-0.1.51\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def check_and_install(package_name):\n",
    "#     try:\n",
    "#         subprocess.check_output(['pip', 'show', package_name])\n",
    "#     except subprocess.CalledProcessError:\n",
    "#         print(f\"{package_name} not found, installing...\")\n",
    "#         subprocess.check_output(['pip', 'install', package_name])\n",
    "#     else:\n",
    "#         print(f\"{package_name} already installed.\")\n",
    "\n",
    "# if 'google.colab' in str(get_ipython()):\n",
    "#     print('Running on CoLab')\n",
    "\n",
    "# check_and_install('langchain_community')\n",
    "# check_and_install('langchain')\n",
    "# check_and_install('openai')\n",
    "# check_and_install('python-dotenv')\n",
    "# check_and_install('tiktoken')\n",
    "# check_and_install('langchain_openai')\n",
    "# check_and_install('google-colab')\n",
    "# !pip install --upgrade langchain\n",
    "import subprocess\n",
    "\n",
    "def check_and_install(package_name):\n",
    "    try:\n",
    "        subprocess.check_output(['pip', 'show', package_name])\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"{package_name} not found, installing...\")\n",
    "        subprocess.check_output(['pip', 'install', package_name])\n",
    "    else:\n",
    "        print(f\"{package_name} already installed.\")\n",
    "\n",
    "check_and_install('langgraph')\n",
    "\n",
    "key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1738283394041,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "RVVk2KjKvBiY",
    "outputId": "341250e1-668c-4be2-d2b3-18d90b3af1fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hi Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 1804,
     "status": "ok",
     "timestamp": 1738283395844,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "Vn0dxaVxwVl4",
    "outputId": "ef9345bb-e957-4f6f-b541-ef85d6eff277"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I don't have access to personal information about you, so I don't know your name. If you'd like to share your name or any other information, feel free to do so!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "\n",
    "response2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1738283498687,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "khW4YDiQxHPE",
    "outputId": "c8474644-3860-4799-aaea-07ff06721a57"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Bob! How can I help you today, Bob?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "response3 = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUvKRjUF36-D"
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84969cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except ImportError as e:\n",
    "    print('Graph visualization not available due to missing dependencies:', e)\n",
    "except AttributeError as e:\n",
    "    print('Graph visualization not available due to an attribute error:', e)\n",
    "except FileNotFoundError as e:\n",
    "    print('Graph visualization not available due to a file not found error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlPCEdUZ8VNX"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1738286610273,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "G9DSgzlK8V9-",
    "outputId": "4e66ecbd-f45b-4031-9e41-c21b5b77408a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi, Charles! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Charles!\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1738286631600,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "0PUDa4YU9Pcv",
    "outputId": "09232ce3-7f9e-4be1-c1a3-5bf15ce88438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Charles! How can I help you today, Charles?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1382,
     "status": "ok",
     "timestamp": 1738286801744,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "8gSiK90V9yMU",
    "outputId": "6ad67a16-30b3-43dd-a900-68c30526b662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't know your name. If you would like to share it, feel free!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1738286916941,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "lHLYvZ1y-M1S",
    "outputId": "6b0974cc-b206-4818-a102-c09b3c70686c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Charles. If there's anything else you'd like to discuss or ask, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8axZSN5_CDp"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEmktTjsABcn"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1738287382488,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "7xd-XOQRAFlo",
    "outputId": "fb78d308-d421-4189-d6dd-947d87b8554e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What brings ye sailin' me way today? Be ye in need of knowledge or perhaps a tale of the Seven Seas? Speak yer mind!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1738287560725,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "-5Grlm_TAxEO",
    "outputId": "fc240442-10c3-4ef9-8156-f3b81572f4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Arrr, ye be callin' yerself Jim! A fine name fer a landlubber or a sea dog alike! What else can I do fer ye today, matey?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyaTEOnrA39E"
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGfLYDOcBBzk"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1313,
     "status": "ok",
     "timestamp": 1738287726590,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "LqY8pot8BRVh",
    "outputId": "e62f17ce-e653-40bb-9a59-c25c7f7b5410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1738287759370,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "FrfjGrSYBgud",
    "outputId": "c620b24c-b94b-41f3-c45b-cfc873f8051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1738288124374,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "4NpjkldpCTD-",
    "outputId": "3af1328f-5e4d-46c6-e1a6-2e4003d61d0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvGJf5AsCiRr"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tNG_oM5F7AW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1738288204006,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "TmSt3pr9DM0d",
    "outputId": "79236ee8-cff6-4faa-ed0a-83b5eb0fc890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. Would you like to share it with me?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1738288281801,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "HcLAhbc-DiXd",
    "outputId": "6fb5b033-31d9-4c4c-96bb-c15664b2ebb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked for the answer to the math problem \"2 + 2.\"\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3218,
     "status": "ok",
     "timestamp": 1738288697981,
     "user": {
      "displayName": "Charles Niswander",
      "userId": "06375059946891921087"
     },
     "user_tz": 300
    },
    "id": "SAg3xNEjEK9s",
    "outputId": "bb2edf1c-fb9e-4fc8-ed90-dc4bc6c92410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hab|ari| Todd|!| H|apa| kuna| ut|ani|:\n",
      "\n",
      "|Kwa| nini| kuku| aliv|uka| bar|abara|?\n",
      "\n",
      "|I|li| kuf|ika| upande| mwingine|!| \n",
      "\n",
      "|Haha|!| ||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNB3hqkyG2Dw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38M_L9j1HDEU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhmj9xNdVCJEuNmZGJbYnC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
