{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11328145",
   "metadata": {},
   "source": [
    "# Prompt Engineering Notebook: Multimodal Prompting\n",
    "*Google Colab–Compatible — Author: ChatGPT (o3) — Date: 2025-07-09*\n",
    "\n",
    "This interactive notebook explores **multimodal prompting**—combining images, audio, and text within a single prompt or pipeline to unlock richer capabilities in modern foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620c31e",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this notebook you will be able to:\n",
    "1. Define multimodal prompting and list common modality pairs (text + image, text + audio, etc.).\n",
    "2. Craft image‑conditioned prompts for vision‑language models (VLMs) such as **BLIP‑2** or GPT‑4o.\n",
    "3. Implement a visual question‑answering (VQA) demo in under 20 lines of code.\n",
    "4. Chain audio transcription (Whisper) with an LLM to build an audio‑aware agent.\n",
    "5. Evaluate multimodal outputs for grounding, faithfulness, and bias.\n",
    "6. Identify policy and safety issues unique to multimedia inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325effda",
   "metadata": {},
   "source": [
    "## 0  | Environment Setup\n",
    "Run the cell below to install lightweight dependencies. Comment out anything you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pillow transformers diffusers\n",
    "# Uncomment for audio examples (heavy)\n",
    "# !pip -q install git+https://github.com/openai/whisper.git --upgrade\n",
    "# !pip -q install torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2297",
   "metadata": {},
   "source": [
    "### Configure API Credentials (Optional)\n",
    "If you have access to the **OpenAI Vision** or **Audio** endpoints, add your key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('🔑 OpenAI API Key (optional): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3e35b",
   "metadata": {},
   "source": [
    "## 1  | What Is Multimodal Prompting?\n",
    "A **multimodal prompt** supplies *multiple input types*—for example, an image *and* text—to a model that natively handles those modalities or combines specialist models in a pipeline.\n",
    "\n",
    "### Typical Patterns\n",
    "| Pattern | Example | Common Models |\n",
    "|---------|---------|---------------|\n",
    "| **Image → Text** | Caption a photo | BLIP‑2, GPT‑4o‑Vision |\n",
    "| **Image + Text → Text** | VQA: *“What color is the car in this image?”* | BLIP‑2, Gemini, LLaVA |\n",
    "| **Text → Image** | *“Generate a logo of a purple owl.”* | Stable Diffusion, DALL·E 3 |\n",
    "| **Audio → Text** | Transcribe a lecture | Whisper, GPT‑4o‑Audio |\n",
    "| **Audio + Text → Text** | Ask follow‑up questions about a recording | Whisper + LLM chain |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e5b18",
   "metadata": {},
   "source": [
    "## 2  | Hands‑On I – Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69a299",
   "metadata": {},
   "source": [
    "> **Explain:** Image captioning converts pixels to a descriptive sentence, providing a gentle intro to vision‑language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, torch\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "blip = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def caption_from_url(url):\n",
    "    raw = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    inputs = processor(raw, return_tensors=\"pt\").to(device)\n",
    "    out = blip.generate(**inputs, max_length=30)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "demo_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/transformer.png\"\n",
    "print(\"Caption:\", caption_from_url(demo_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17561f",
   "metadata": {},
   "source": [
    "**Exercise 🖼️**: Replace `demo_url` with any image link or upload a file via Colab sidebar, then caption it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fe108",
   "metadata": {},
   "source": [
    "## 3  | Hands‑On II – Visual Question Answering (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c867d",
   "metadata": {},
   "source": [
    "We'll reuse BLIP‑2's Q&A head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "vqa_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "processor_vqa = Blip2Processor.from_pretrained(vqa_name)\n",
    "vqa_model = Blip2ForConditionalGeneration.from_pretrained(vqa_name, device_map=\"auto\").eval()\n",
    "\n",
    "def vqa(url, question):\n",
    "    img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    inputs = processor_vqa(images=img, text=question, return_tensors=\"pt\").to(device)\n",
    "    res = vqa_model.generate(**inputs, max_length=30)\n",
    "    return processor_vqa.decode(res[0], skip_special_tokens=True)\n",
    "\n",
    "print(vqa(demo_url, \"What component is highlighted?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ad9de",
   "metadata": {},
   "source": [
    "**Exercise 🔍**: Ask two different questions about the same image. Compare accuracy as questions become more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d06a9",
   "metadata": {},
   "source": [
    "### Prompt Engineering Tips for VLMs\n",
    "1. **Ground the question**: Reference spatial cues (\"bottom left\", \"top center\").\n",
    "2. **Provide context**: *“In this technical diagram...”* improves domain grounding.\n",
    "3. **Chain‑of‑thought**: Some VLMs support step‑by‑step reasoning when asked explicitly.\n",
    "4. **System messages**: In GPT‑4o, prepend a system role that instructs concise answers with citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b75c8",
   "metadata": {},
   "source": [
    "## 4  | Hands‑On III – Audio‑Aware Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Upload a short WAV/MP3 clip via Colab and set the filename below\n",
    "audio_file = \"sample_audio.mp3\"  # change me\n",
    "\n",
    "try:\n",
    "    import whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_file, fp16=False)\n",
    "    transcript = result['text']\n",
    "except Exception as e:\n",
    "    transcript = \"(transcript unavailable — install whisper & upload an audio file)\"\n",
    "print(\"Transcript:\", transcript[:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d21c5f",
   "metadata": {},
   "source": [
    "### Chaining with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5abbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import shorten\n",
    "question = \"Summarize the key points from this talk in three bullet points.\"\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI(temperature=0)\n",
    "    answer = llm(f\"\"\"Use the transcript below to answer the question.\n",
    "\n",
    "Transcript:\n",
    "\"\"\"{shorten(transcript, 400)}\"\"\"\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "else:\n",
    "    answer = \"(stub) Key points: 1) Example 2) Example 3) Example\"\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66f894",
   "metadata": {},
   "source": [
    "**Mini‑project 📑**: Build a *podcast assistant* that transcribes an episode, chunks it, and allows follow‑up Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7b7b3",
   "metadata": {},
   "source": [
    "## 5  | Evaluation & Grounding Metrics\n",
    "- **Grounding**: Does the answer reference actual visual/audio evidence?\n",
    "- **Faithfulness**: No invented details beyond the supplied media.\n",
    "- **Relevance**: Retrieved frames/chunks aligned with question scope.\n",
    "- **Bias & Fairness**: Check demographic attributes in captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06afde0",
   "metadata": {},
   "source": [
    "## 6  | Safety & Policy Concerns\n",
    "- **Sensitive imagery**: NSFW, violent, or private images require filters.\n",
    "- **Faces & PII**: Avoid identifying real individuals without consent.\n",
    "- **Audio privacy**: Recordings may contain personal data.\n",
    "- **Copyright**: Ensure you have rights to distribute images/audio used in demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae373e50",
   "metadata": {},
   "source": [
    "## Assignment 🎓\n",
    "Design a multimodal tutor that helps students learn geography by:\n",
    "1. Accepting an image of a world map section.\n",
    "2. Answering *three* progressively harder questions about the region.\n",
    "3. Providing follow‑up resources with image URLs.\n",
    "4. Logging each interaction with timestamp and question difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700917c",
   "metadata": {},
   "source": [
    "## Further Reading & Resources\n",
    "- Li et al., *BLIP‑2: Bootstrapping Language‑Image Pre‑training* (2023)\n",
    "- OpenAI *Vision* & *Audio* docs (GPT‑4o)\n",
    "- *LLaVA*: Large‑Language‑and‑Vision Assistant (2023)\n",
    "- Diffusers library (Hugging Face) for text‑to‑image prompting\n",
    "- Microsoft *Kosmos‑2* multimodal grounding"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
