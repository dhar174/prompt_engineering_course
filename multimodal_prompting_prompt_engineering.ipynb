{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11328145",
   "metadata": {},
   "source": [
    "# Prompt Engineering Notebook: Multimodal Prompting\n",
    "*Google Colabâ€“Compatible â€” Author: ChatGPT (o3) â€” Date: 2025-07-09*\n",
    "\n",
    "This interactive notebook explores **multimodal prompting**â€”combining images, audio, and text within a single prompt or pipeline to unlock richer capabilities in modern foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620c31e",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this notebook you will be able to:\n",
    "1. Define multimodal prompting and list common modality pairs (textÂ +Â image, textÂ +Â audio, etc.).\n",
    "2. Craft imageâ€‘conditioned prompts for visionâ€‘language models (VLMs) such as **BLIPâ€‘2** or GPTâ€‘4o.\n",
    "3. Implement a visualâ€¯questionâ€‘answering (VQA) demo in under 20 lines of code.\n",
    "4. Chain audio transcription (Whisper) with an LLM to build an audioâ€‘aware agent.\n",
    "5. Evaluate multimodal outputs for grounding, faithfulness, and bias.\n",
    "6. Identify policy and safety issues unique to multimedia inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325effda",
   "metadata": {},
   "source": [
    "## 0â€¯Â |Â Environment Setup\n",
    "Run the cell below to install lightweight dependencies. Comment out anything you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pillow transformers diffusers\n",
    "# Uncomment for audio examples (heavy)\n",
    "# !pip -q install git+https://github.com/openai/whisper.git --upgrade\n",
    "# !pip -q install torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2297",
   "metadata": {},
   "source": [
    "### Configure API Credentials (Optional)\n",
    "If you have access to the **OpenAI Vision** or **Audio** endpoints, add your key below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('ğŸ”‘ OpenAI API Key (optional): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3e35b",
   "metadata": {},
   "source": [
    "## 1â€¯Â |Â WhatÂ IsÂ Multimodalâ€¯Prompting?\n",
    "A **multimodal prompt** supplies *multiple input types*â€”for example, an image *and* textâ€”to a model that natively handles those modalities or combines specialist models in a pipeline.\n",
    "\n",
    "### Typical Patterns\n",
    "| Pattern | Example | Common Models |\n",
    "|---------|---------|---------------|\n",
    "| **Imageâ€¯â†’â€¯Text** | Caption a photo | BLIPâ€‘2, GPTâ€‘4oâ€‘Vision |\n",
    "| **Imageâ€¯+â€¯Textâ€¯â†’â€¯Text** | VQA: *â€œWhat color is the car in this image?â€* | BLIPâ€‘2, Gemini, LLaVA |\n",
    "| **Textâ€¯â†’â€¯Image** | *â€œGenerate a logo of a purple owl.â€* | Stable Diffusion, DALLÂ·Eâ€¯3 |\n",
    "| **Audioâ€¯â†’â€¯Text** | Transcribe a lecture | Whisper, GPTâ€‘4oâ€‘Audio |\n",
    "| **Audioâ€¯+â€¯Textâ€¯â†’â€¯Text** | Ask followâ€‘up questions about a recording | Whisperâ€¯+â€¯LLM chain |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e5b18",
   "metadata": {},
   "source": [
    "## 2â€¯Â |Â Handsâ€‘OnÂ IÂ â€“Â Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69a299",
   "metadata": {},
   "source": [
    "> **Explain:** Image captioning converts pixels to a descriptive sentence, providing a gentle intro to visionâ€‘language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests, torch\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "blip = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def caption_from_url(url):\n",
    "    raw = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    inputs = processor(raw, return_tensors=\"pt\").to(device)\n",
    "    out = blip.generate(**inputs, max_length=30)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "demo_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/transformer.png\"\n",
    "print(\"Caption:\", caption_from_url(demo_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17561f",
   "metadata": {},
   "source": [
    "**ExerciseÂ ğŸ–¼ï¸**: Replace `demo_url` with any image link or upload a file via Colab sidebar, then caption it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fe108",
   "metadata": {},
   "source": [
    "## 3â€¯Â |Â Handsâ€‘OnÂ IIÂ â€“Â VisualÂ Questionâ€¯Answering (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c867d",
   "metadata": {},
   "source": [
    "We'll reuse BLIPâ€‘2's Q&A head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "vqa_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "processor_vqa = Blip2Processor.from_pretrained(vqa_name)\n",
    "vqa_model = Blip2ForConditionalGeneration.from_pretrained(vqa_name, device_map=\"auto\").eval()\n",
    "\n",
    "def vqa(url, question):\n",
    "    img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    inputs = processor_vqa(images=img, text=question, return_tensors=\"pt\").to(device)\n",
    "    res = vqa_model.generate(**inputs, max_length=30)\n",
    "    return processor_vqa.decode(res[0], skip_special_tokens=True)\n",
    "\n",
    "print(vqa(demo_url, \"What component is highlighted?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ad9de",
   "metadata": {},
   "source": [
    "**Exerciseâ€¯ğŸ”**: Ask two different questions about the same image. Compare accuracy as questions become more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d06a9",
   "metadata": {},
   "source": [
    "### Prompt Engineering Tips for VLMs\n",
    "1. **Ground the question**: Reference spatial cues (\"bottom left\", \"top center\").\n",
    "2. **Provide context**: *â€œIn this technical diagram...â€* improves domain grounding.\n",
    "3. **Chainâ€‘ofâ€‘thought**: Some VLMs support stepâ€‘byâ€‘step reasoning when asked explicitly.\n",
    "4. **System messages**: In GPTâ€‘4o, prepend a system role that instructs concise answers with citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b75c8",
   "metadata": {},
   "source": [
    "## 4â€¯Â |Â Handsâ€‘OnÂ IIIÂ â€“Â Audioâ€‘Aware Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Upload a short WAV/MP3 clip via Colab and set the filename below\n",
    "audio_file = \"sample_audio.mp3\"  # change me\n",
    "\n",
    "try:\n",
    "    import whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_file, fp16=False)\n",
    "    transcript = result['text']\n",
    "except Exception as e:\n",
    "    transcript = \"(transcript unavailable â€” install whisper & upload an audio file)\"\n",
    "print(\"Transcript:\", transcript[:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d21c5f",
   "metadata": {},
   "source": [
    "### Chaining with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5abbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import shorten\n",
    "question = \"Summarize the key points from this talk in three bullet points.\"\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI(temperature=0)\n",
    "    answer = llm(f\"\"\"Use the transcript below to answer the question.\n",
    "\n",
    "Transcript:\n",
    "\"\"\"{shorten(transcript, 400)}\"\"\"\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "else:\n",
    "    answer = \"(stub) Key points: 1) Example 2) Example 3) Example\"\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66f894",
   "metadata": {},
   "source": [
    "**Miniâ€‘project ğŸ“‘**: Build a *podcast assistant* that transcribes an episode, chunks it, and allows followâ€‘up Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7b7b3",
   "metadata": {},
   "source": [
    "## 5â€¯Â |Â Evaluation & Grounding Metrics\n",
    "- **Grounding**: Does the answer reference actual visual/audio evidence?\n",
    "- **Faithfulness**: No invented details beyond the supplied media.\n",
    "- **Relevance**: Retrieved frames/chunks aligned with question scope.\n",
    "- **Bias & Fairness**: Check demographic attributes in captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06afde0",
   "metadata": {},
   "source": [
    "## 6â€¯Â |Â Safety & Policy Concerns\n",
    "- **Sensitive imagery**: NSFW, violent, or private images require filters.\n",
    "- **Faces & PII**: Avoid identifying real individuals without consent.\n",
    "- **Audio privacy**: Recordings may contain personal data.\n",
    "- **Copyright**: Ensure you have rights to distribute images/audio used in demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae373e50",
   "metadata": {},
   "source": [
    "## AssignmentÂ ğŸ“\n",
    "Design a multimodal tutor that helps students learn geography by:\n",
    "1. Accepting an image of a world map section.\n",
    "2. Answering *three* progressively harder questions about the region.\n",
    "3. Providing followâ€‘up resources with image URLs.\n",
    "4. Logging each interaction with timestamp and question difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700917c",
   "metadata": {},
   "source": [
    "## Further Reading & Resources\n",
    "- Li etÂ al., *BLIPâ€‘2: Bootstrapping Languageâ€‘Image Preâ€‘training* (2023)\n",
    "- OpenAI *Vision* & *Audio* docs (GPTâ€‘4o)\n",
    "- *LLaVA*: Largeâ€‘Languageâ€‘andâ€‘Vision Assistant (2023)\n",
    "- Diffusers library (Huggingâ€¯Face) for textâ€‘toâ€‘image prompting\n",
    "- Microsoft *Kosmosâ€‘2* multimodal grounding"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
