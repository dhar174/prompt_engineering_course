{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a21cb69",
   "metadata": {},
   "source": [
    "# Prompt Engineering Notebook: Retrievalâ€‘Augmented Generation (RAG)\n",
    "*Google Colabâ€“Compatible â€” Author: ChatGPT (o3) â€” Date: 2025-07-09*\n",
    "\n",
    "This interactive notebook demystifies **Retrievalâ€‘Augmented Generation**â€”injecting external knowledge into languageâ€‘model prompts at runtime to boost accuracy, reduce hallucination, and keep responses fresh.\n",
    "\n",
    "> ðŸ§‘â€ðŸ« *Pedagogical design note:* Each section follows the **explainâ€‘demoâ€‘exercise** pattern, with progressively richer challenges and builtâ€‘in reflection checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23fd12",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this notebook you will be able to:\n",
    "1. Explain the standard RAG pipeline and why it mitigates hallucinations.\n",
    "2. Build a toy document store with embeddings + FAISS and perform similarity search.\n",
    "3. Craft dynamic prompts that combine retrieved context with user questions.\n",
    "4. Evaluate RAG responses for relevance and factual grounding using simple metrics.\n",
    "5. Experiment with chunking, hybrid search, and citation formatting.\n",
    "6. Identify safety & bias pitfalls unique to RAG systems and propose mitigations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46138c4",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "Run the next cell to install lightweight dependencies. Feel free to commentâ€‘out packages you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9330a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain openai faiss-cpu python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec9eff",
   "metadata": {},
   "source": [
    "### Configure API Credentials\n",
    "Enter your OpenAI API key below (optionalâ€”offline demo stubs will be used if absent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('ðŸ”‘ OpenAI API Key (optional): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59936aba",
   "metadata": {},
   "source": [
    "## 1. Why Retrievalâ€‘Augmented Generation?\n",
    "Large language models (LLMs) possess vast latent knowledge but **forget specifics** and **freeze in time**. RAG connects them to an upâ€‘toâ€‘date knowledge base **at inferenceâ€‘time**:\n",
    "```text\n",
    "User Query â”€â–¶ Retriever â”€â–¶ Relevant Chunks â”€â”\n",
    "                                   â¬‡         â”‚\n",
    "                       [Prompt Template]      â”‚\n",
    "                                   â¬‡         â”‚\n",
    "                                  LLM â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551da56f",
   "metadata": {},
   "source": [
    "### Key Benefits\n",
    "1. **FreshnessÂ ðŸ“…** â€“ Inject new documents without retraining.\n",
    "2. **ExplainabilityÂ ðŸ”** â€“ Provide citations for retrieved passages.\n",
    "3. **EfficiencyÂ âš¡** â€“ Smaller models can perform expert tasks when paired with domain docs.\n",
    "4. **ControlÂ ðŸ› ï¸** â€“ Curate the knowledge corpus to steer model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a437f19",
   "metadata": {},
   "source": [
    "## 2. Handsâ€‘On PartÂ I â€“ Create a Toy Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "sample_docs = {\n",
    "    \"transformers\": \"\"\"Transformers are neural network architectures that rely on selfâ€‘attention. \n",
    "They have become the backbone of modern NLP.\"\"\",\n",
    "    \"vector-db\": \"\"\"FAISS is a library for efficient similarity search and clustering of dense vectors.\"\"\",\n",
    "    \"rag\": \"\"\"Retrievalâ€‘Augmented Generation pipelines retrieve relevant chunks from external documents \n",
    "and feed them into the prompt of an LLM.\"\"\"\n",
    "}\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=10)\n",
    "docs = []\n",
    "for title, text in sample_docs.items():\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(chunk)\n",
    "print(f\"ðŸ“„ {len(docs)} chunks created:\")\n",
    "for d in docs:\n",
    "    print('-', d[:60] + ('...' if len(d) > 60 else ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e43a80",
   "metadata": {},
   "source": [
    "### 2.1 Embed & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    # Offline stub embeds by hashing\n",
    "    import numpy as np, hashlib\n",
    "    class HashEmbeddings:\n",
    "        def embed_documents(self, texts):\n",
    "            return [np.array([int(hashlib.md5(t.encode()).hexdigest(),16)%997]) for t in texts]\n",
    "    embeddings = HashEmbeddings()\n",
    "\n",
    "db = FAISS.from_texts(docs, embeddings)\n",
    "print('âœ… Vector store ready â€”', db.index.ntotal, 'vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5b076",
   "metadata": {},
   "source": [
    "### 2.2 Similarity Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What library helps with similarity search for embeddings?\"\n",
    "docs_ret = db.similarity_search(query, k=2)\n",
    "for i,d in enumerate(docs_ret,1):\n",
    "    print(f\"Doc {i} â–¶\", d.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bacb8f8",
   "metadata": {},
   "source": [
    "**ExerciseÂ ðŸ“**: Change `query` and reâ€‘run. Observe retrieved snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b55709",
   "metadata": {},
   "source": [
    "## 3. Handsâ€‘On PartÂ II â€“ Plug Retrieval into Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ae680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(question, contexts):\n",
    "    joined = \"\\n---\\n\".join(contexts)\n",
    "    return f\"\"\"Answer the question using the context below. Cite sources using [docÂ #].\n",
    "\n",
    "Context:\n",
    "{joined}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "def ask_rag(question, k=3):\n",
    "    ctx = [d.page_content for d in db.similarity_search(question, k=k)]\n",
    "    prompt = format_prompt(question, ctx)\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        from langchain.llms import OpenAI\n",
    "        llm = OpenAI(temperature=0)\n",
    "        return llm(prompt)\n",
    "    else:\n",
    "        # Offline stub: echo context keywords\n",
    "        return \"Stubâ€‘LLM answer based on retrieved info: \" + ', '.join(ctx)\n",
    "\n",
    "print(ask_rag(\"Explain what FAISS does.\", k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b637b",
   "metadata": {},
   "source": [
    "**CheckpointÂ ðŸ”Ž**: Does the answer cite sources? If not, tweak `format_prompt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764cf0c",
   "metadata": {},
   "source": [
    "## 4. Experiment â€“ Chunk Size vs. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "sizes = [32, 64, 128, 256]\n",
    "recall_scores=[]\n",
    "for cs in sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=cs, chunk_overlap=10)\n",
    "    chunks = sum(len(splitter.split_text(t)) for t in sample_docs.values())\n",
    "    recall_scores.append(min(1, 80/cs))  # fake metric for demo\n",
    "\n",
    "plt.plot(sizes, recall_scores, marker='o')\n",
    "plt.title(\"Effect of Chunk Size on (Toy) Recall\")\n",
    "plt.xlabel(\"Chunk Size (chars)\")\n",
    "plt.ylabel(\"Recall (proxy)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e6bf5",
   "metadata": {},
   "source": [
    "> **ReflectionÂ ðŸ’¡**: Larger chunks capture more context but may dilute retrieval precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e4554",
   "metadata": {},
   "source": [
    "## 5. Evaluating RAG Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b0885",
   "metadata": {},
   "source": [
    "*Automatic heuristics*\n",
    "- **Context precision**: % of retrieved chunks actually used in answer.\n",
    "- **Citation accuracy**: Do cited docs support the claim?\n",
    "- **Answer faithfulness**: Compare with ground truth via LLM verifier or overlap metrics.\n",
    "\n",
    "*Manual checklist*\n",
    "- Factual correctness?\n",
    "- Is necessary info missing?\n",
    "- Tone & completeness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b02c3f",
   "metadata": {},
   "source": [
    "## 6. Going Further\n",
    "- **Hybrid retrieval**: combine dense (embeddings) + sparse (BM25) search.\n",
    "- **Graphâ€‘based retrieval**: store doc relationships and traverse paths.\n",
    "- **Multiâ€‘hop RAG**: iterative retrieval to answer complex queries.\n",
    "- **Streaming RAG**: produce partial answers while fetching more chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c838b",
   "metadata": {},
   "source": [
    "## 7. Safety & Bias Considerations\n",
    "- **Data poisoning**: Malicious docs may steer outputs â†’ Validate sources.\n",
    "- **Privacy leaks**: Retrieved chunks might expose sensitive info â†’ Access control.\n",
    "- **Confabulations**: LLM may still hallucinate even with context â†’ Force citation and compare.\n",
    "- **License compliance**: Ensure you have rights to redistribute retrieved content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33ba38",
   "metadata": {},
   "source": [
    "## AssignmentÂ ðŸ“š\n",
    "1. Build a miniâ€‘knowledge base from **any 3 Wikipedia articles** of your choice.\n",
    "2. Implement a RAG function that answers *five* trivia questions about those topics.\n",
    "3. Report precision, citation accuracy, and one failure case with analysis.\n",
    "4. *(Stretch)* Swap FAISS for **Chroma** or **Weaviate** and compare retrieval latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a7dd1",
   "metadata": {},
   "source": [
    "## Further Reading & Tools\n",
    "- Lewis etÂ al., *Retrievalâ€‘Augmented Generation for Knowledgeâ€‘Intensive NLP* (2020)\n",
    "- LlamaIndex (GPT Index) and LangChain RAG templates\n",
    "- **Haystack** framework (deepset) for production RAG pipelines\n",
    "- OpenAI Cookbook: *Evaluating RAG pipelines*\n",
    "- Papers with Code: <https://paperswithcode.com/task/retrieval-augmented-generation>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
