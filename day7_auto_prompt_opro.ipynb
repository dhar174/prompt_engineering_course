{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5744f0a8",
   "metadata": {},
   "source": [
    "# AutoPrompt & OPRO: Automated Gradient‑Based Prompt Engineering\n",
    "This notebook walks through two state‑of‑the‑art automated prompt discovery methods:\n",
    "1. **AutoPrompt** – greedily inserts tokens that maximise a supervised objective.\n",
    "2. **OPRO (One‑Prompt Reasoning Optimisation)** – searches for chain‑of‑thought prompts that maximise *reasoning* reward.\n",
    "\n",
    "*Pedagogical goal:* Show how **model gradients** and **search/optimisation** can find high‑performing prompts without human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e3433",
   "metadata": {},
   "source": [
    "## 1. Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade transformers datasets trl einops accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb9e95",
   "metadata": {},
   "source": [
    "## 2. AutoPrompt on Sentiment\n",
    "We use the original sentiment‑analysis task from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools, random, re, math\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "dataset = load_dataset('sst2', split='train[:100]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9469c",
   "metadata": {},
   "source": [
    "### Helper: compute accuracy of a prompt template\n",
    "Template: `[CLS] {prompt} {sentence} It was [MASK]. [SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74442023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_accuracy(prompt_tokens):\n",
    "    correct=0\n",
    "    for ex in dataset:\n",
    "        sent=ex['sentence']\n",
    "        label=ex['label']\n",
    "        text = tok.cls_token + ' ' + ' '.join(prompt_tokens) + ' ' + sent + ' It was ' + tok.mask_token + '.'\n",
    "        inp = tok(text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            logits = mlm(**inp).logits\n",
    "        mask_ix = (inp['input_ids'][0]==tok.mask_token_id).nonzero()[0]\n",
    "        probs = logits[0, mask_ix].softmax(-1)\n",
    "        pred = probs[tok.convert_tokens_to_ids('great')] > probs[tok.convert_tokens_to_ids('terrible')]\n",
    "        if pred.item() == label: correct +=1\n",
    "    return correct/len(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a699",
   "metadata": {},
   "source": [
    "### AutoPrompt loop (simplified – one position at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e12d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(tok.get_vocab().keys())\n",
    "prompt=['it']*3  # 3‑token prompt stub\n",
    "for pos in range(len(prompt)):\n",
    "    best=(None,0)\n",
    "    subset=random.sample(vocab,500)  # small subset for speed\n",
    "    for w in subset:\n",
    "        trial=prompt.copy(); trial[pos]=w\n",
    "        acc=prompt_accuracy(trial)\n",
    "        if acc>best[1]: best=(w,acc)\n",
    "    prompt[pos]=best[0]\n",
    "    print(f'Pos {pos} -> {best}')\n",
    "print('Auto‑discovered prompt:', prompt)\n",
    "print('Final acc:', prompt_accuracy(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33055d",
   "metadata": {},
   "source": [
    "## 3. OPRO for arithmetic chain‑of‑thought\n",
    "OPRO uses a **reward model** (here: correctness of arithmetic) and **search over prompts**.\n",
    "*We implement a tiny evolutionary hill‑climb for brevity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "lm=AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "def evaluate(prompt):\n",
    "    q='What is 13 plus 24?'\n",
    "    full=prompt+q\n",
    "    out=tok(full, return_tensors='pt').to(lm.device)\n",
    "    gen=lm.generate(**out, max_new_tokens=10)\n",
    "    ans=tok.decode(gen[0], skip_special_tokens=True).split('\\n')[-1]\n",
    "    return abs(int(re.findall(r'\\d+', ans)[0]) - 37)  # reward = distance to correct\n",
    "\n",
    "population=['Let us solve step by step: 13 + 24 =',\n",
    "           'First compute 13+24 to get',\n",
    "           'Answer to 13 plus 24 is']\n",
    "for gen in range(5):\n",
    "    scored=[(p,evaluate(p)) for p in population]\n",
    "    scored.sort(key=lambda x:x[1])\n",
    "    print(f'Gen {gen} best:', scored[0])\n",
    "    # mutate top 2\n",
    "    parents=[p for p,_ in scored[:2]]\n",
    "    population=parents+ [p+' therefore' for p in parents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb5516",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "- **AutoPrompt** reveals *heuristic tokens* that implicitly steer a frozen LM.\n",
    "- **OPRO** treats prompt text as a policy optimised by search.\n",
    "- Both require *no gradient updates to model weights*, yet achieve surprising gains."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
