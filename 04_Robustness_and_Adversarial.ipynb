{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd36be1c",
   "metadata": {},
   "source": [
    "# Notebook 4 – Robustness, Debugging & Adversarial Testing\n",
    "Objectives:\n",
    "1. Capture failure cases and create a *failure library*\n",
    "2. Perform iterative prompt debugging\n",
    "3. Launch adversarial attacks using `promptbench`\n",
    "4. Compute a simple **Prompt‑Robustness Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install promptbench==0.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8ecc0",
   "metadata": {},
   "source": [
    "## 1. Failure‑Logging Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, uuid, datetime, pathlib\n",
    "def log_failure(prompt, input_text, output_text, issue):\n",
    "    log = {\n",
    "        'id': str(uuid.uuid4()),\n",
    "        'timestamp': datetime.datetime.utcnow().isoformat(),\n",
    "        'prompt': prompt,\n",
    "        'input': input_text,\n",
    "        'output': output_text,\n",
    "        'issue': issue\n",
    "    }\n",
    "    pathlib.Path('failure_logs').mkdir(exist_ok=True)\n",
    "    with open(f'failure_logs/{log[\"id\"]}.json', 'w') as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "    return log['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ec38f",
   "metadata": {},
   "source": [
    "## 2. Adversarial Testing Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptbench import attack\n",
    "test_prompt = 'Translate the following English text to French: {input}'\n",
    "adv_examples = attack.generate_adversarial_examples(test_prompt, n=3)\n",
    "adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18ffb0",
   "metadata": {},
   "source": [
    "## 3. Prompt‑Robustness Score (Toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7508e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_score(original_outputs, adversarial_outputs):\n",
    "    matches = sum(o == a for o, a in zip(original_outputs, adversarial_outputs))\n",
    "    return matches / len(original_outputs)\n",
    "\n",
    "# Placeholder example\n",
    "robustness_score(['Bonjour', 'Salut'], ['Bonjour', 'Salut'])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
