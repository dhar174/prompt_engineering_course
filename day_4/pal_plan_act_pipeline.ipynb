{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982538db",
   "metadata": {},
   "source": [
    "# PAL & Plan\u2011Then\u2011Act Pipeline\n",
    "LLM writes Python, we execute it, then ask LLM to explain. The Plan-and-Act Loop iteratively plans steps, executes them, and refines future actions using feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "security_warning",
   "metadata": {},
   "source": [
    "## ⚠️ **SECURITY WARNING** ⚠️\n",
    "\n",
    "**This notebook contains significant security risks and should only be used in a secure, isolated environment for educational purposes.**\n",
    "\n",
    "### Security Risks:\n",
    "- **Code Execution Risk**: This notebook uses `exec()` to run Python code generated by an LLM\n",
    "- **No Sandboxing**: The executed code has full access to your system, files, and network\n",
    "- **Potential for Malicious Code**: LLMs can generate harmful code including:\n",
    "  - File system operations (read, write, delete files)\n",
    "  - Network requests (data exfiltration, downloading malware)\n",
    "  - System commands (potentially compromising your system)\n",
    "  - Memory operations that could cause crashes\n",
    "\n",
    "### Safety Recommendations:\n",
    "1. **Only run in isolated environments** (containers, VMs, sandboxed environments)\n",
    "2. **Never run on production systems** or systems with sensitive data\n",
    "3. **Review generated code** before execution when possible\n",
    "4. **Consider alternatives** like AST-based evaluation for simple expressions\n",
    "5. **Implement input validation** and restricted execution environments for production use\n",
    "6. **Consider sandboxing approaches** for production use:\n",
    "   - Restrict Python builtins: `exec(code, {'__builtins__': {}}, {})`\n",
    "   - Use RestrictedPython for more sophisticated restrictions\n",
    "   - Run in containerized or virtualized environments\n",
    "\n",
    "**For production applications, consider using safer alternatives like RestrictedPython, ast.literal_eval() for simple expressions, or containerized execution environments.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install openai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, ipywidgets as w, contextlib, io, traceback, re, warnings\n",
    "from IPython.display import display, Markdown\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'sk-')\n",
    "\n",
    "problem=w.Textarea(value='What is the factorial of 8?',description='Problem:',layout=w.Layout(width='100%',height='60px'))\n",
    "run_btn=w.Button(description='Run PAL')\n",
    "out=w.Output()\n",
    "\n",
    "PAL_SYS='''You are an assistant that solves problems by first planning then writing Python.\\nRespond with three fenced blocks plan/python/answer.''' \n",
    "\n",
    "last_plan=''; last_code=''\n",
    "\n",
    "def run(_):\n",
    "    global last_plan, last_code\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        r=openai.ChatCompletion.create(model='gpt-4o-mini',\n",
    "            messages=[{'role':'system','content':PAL_SYS},{'role':'user','content':problem.value}],\n",
    "            temperature=0.3,max_tokens=400)\n",
    "        txt=r.choices[0].message.content\n",
    
    "        plan_match=re.search(r'```plan\\n(.*?)```', txt, re.DOTALL)\n",
    "        last_plan=plan_match.group(1).strip() if plan_match else ''\n",
    "        display(Markdown('### LLM Response\\n'+txt))\n",
    "        if '```python' not in txt:\n",
    "            print('No code block.');return\n",
    "        code_match=re.search(r'```python\\n(.*?)```', txt, re.DOTALL)\n",
    "        if not code_match:\n",
    "            print('No code block.');return\n",
    "        code=code_match.group(1)\n",
    "        last_code=code\n",
    "        buf=io.StringIO()\n",
    "        try:\n",
    "            with contextlib.redirect_stdout(buf):\n",
    "                warnings.warn(\n",
    "                    \"⚠️ SECURITY RISK: Executing LLM-generated code without sandboxing. \"\n",
    "                    \"This is dangerous and should only be used in isolated environments.\",\n",
    "                    UserWarning,\n",
    "                    stacklevel=2\n",
    "                )\n",
    "                exec(code, {})\n",
    "            out_str=buf.getvalue().strip()\n",
    "        except Exception:\n",
    "            out_str=traceback.format_exc()\n",
    "        print('--- Code Output ---');print(out_str)\n",
    "        exp=openai.ChatCompletion.create(model='gpt-4o-mini',\n",
    "            messages=[{'role':'system','content':'Explain the result.'},{'role':'user','content':out_str}],\n",
    "            temperature=0.2,max_tokens=150)\n",
    "        display(Markdown('### Explanation\\n'+exp.choices[0].message.content))\n",
    "run_btn.on_click(run)\n",
    "display(w.VBox([problem, run_btn, out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Plan ---')\n",
    "print(last_plan)\n",
    "print('--- Python Code ---')\n",
    "print(last_code)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
