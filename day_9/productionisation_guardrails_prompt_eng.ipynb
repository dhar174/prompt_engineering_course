{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf6b6d6",
   "metadata": {},
   "source": [
    "\n",
    "# 🛡️ Productionisation & Guardrails for Prompt Engineering (2025)\n",
    "\n",
    "**Prompt Engineering — Comprehensive Colab Notebook**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "1. **Define** what “production‑ready” means for LLM pipelines.  \n",
    "2. **Implement** guardrails for schema validation, safety, and deterministic outputs.  \n",
    "3. **Monitor & evaluate** live traffic with logging, metrics, and offline tests.  \n",
    "4. **Compare** open‑source guardrail frameworks (GuardrailsAI, LangChain Validation, Pydantic).  \n",
    "5. **Design** fallback and escalation strategies (retrieval, function calls, backup models).  \n",
    "6. **Balance** latency, cost, and risk in real‑world deployments.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da4cf8",
   "metadata": {},
   "source": [
    "\n",
    "## ⏳ Table of Contents\n",
    "1. [Introduction](#intro)  \n",
    "2. [Colab Setup](#setup)  \n",
    "3. [Pipeline Blueprint](#blueprint)  \n",
    "4. [Schema‑First Prompting](#schema)  \n",
    "5. [Safety & Content Filters](#safety)  \n",
    "6. [Monitoring & Logging](#monitor)  \n",
    "7. [Offline Evaluation Harness](#eval)  \n",
    "8. [Fallback & Graceful Degradation](#fallback)  \n",
    "9. [Cost / Latency Engineering](#cost)  \n",
    "10. [Exercises](#ex)  \n",
    "11. [Further Reading](#read)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911682a0",
   "metadata": {},
   "source": [
    "\n",
    "<a id='intro'></a>\n",
    "## 1️⃣ Introduction — Why Guardrails?\n",
    "\n",
    "When prompts leave the lab and power **customer‑facing features**, the stakes rise:\n",
    "\n",
    "* **Unbounded output** can break parsers, UIs, or downstream code.\n",
    "* **Safety violations** (hate, self‑harm, personal data leaks) can harm users.\n",
    "* **Hallucinations** undermine trust and create legal liabilities.\n",
    "* **Latency spikes** and **cost overruns** destroy SLAs and budgets.\n",
    "\n",
    "Guardrails are *contracts* 🌐—explicit constraints enforced at generation time (or immediately after) to ensure outputs are **valid, safe, and useful**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354df45",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2️⃣ Colab Setup — Install Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core deps\n",
    "!pip -q install --upgrade openai==1.31.0 guardrails-ai==0.4.5 langchain-core==0.2.0                   python-dotenv pydantic==2.7.1 rich --progress-bar off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35c711",
   "metadata": {},
   "source": [
    "\n",
    "<a id='blueprint'></a>\n",
    "## 3️⃣ Pipeline Blueprint\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Inference API\n",
    "        A[User Request] -->|Prompt| B[LLM]\n",
    "        B --> C{{Guardrails}}\n",
    "    end\n",
    "    C -->|Valid| D[Post‑Processor]\n",
    "    C -->|Violation| E[Fallback / Error Flow]\n",
    "    D --> F[Cache + DB Logs]\n",
    "    E --> F\n",
    "    F --> G[Analytics / Monitoring]\n",
    "```\n",
    "\n",
    "> **Guardrails** sit *between* raw model output and the rest of your stack.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132524e",
   "metadata": {},
   "source": [
    "\n",
    "<a id='schema'></a>\n",
    "## 4️⃣ Schema‑First Prompting with GuardrailsAI\n",
    "\n",
    "We’ll ask the model for a JSON *Product Review Summary* with strict keys.\n",
    "\n",
    "```python\n",
    "from guardrails import Guard\n",
    "import openai, os, json, rich, textwrap, tempfile\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_KEY\"\n",
    "\n",
    "schema = '''\n",
    "<rail version=\"0.6\">\n",
    "<output>\n",
    "    <object name=\"review_summary\">\n",
    "        <string name=\"sentiment\" enum=\"positive,neutral,negative\"/>\n",
    "        <string name=\"pros\" max_tokens=\"40\"/>\n",
    "        <string name=\"cons\" max_tokens=\"40\"/>\n",
    "    </object>\n",
    "</output>\n",
    "<prompt>\n",
    "Summarize the following product review. Only fill the JSON object.\n",
    "</prompt>\n",
    "</rail>\n",
    "'''\n",
    "guard = Guard.from_rail_string(schema)\n",
    "\n",
    "review = \"\"\"I bought this headset for gaming. Audio quality is mind‑blowing and the mic\n",
    "is crystal clear, but after two hours my ears hurt. Battery life is okay.\"\"\"\n",
    "\n",
    "prompt = guard.base_prompt.format(review)\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "validated = guard.parse(response.choices[0].message.content)\n",
    "validated\n",
    "```\n",
    "\n",
    "If the model strays from the schema, Guardrails will **auto‑re‑prompt** up to *n* retries, then raise an error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc233e6a",
   "metadata": {},
   "source": [
    "\n",
    "<a id='safety'></a>\n",
    "## 5️⃣ Safety & Content Filters\n",
    "\n",
    "### 5.1 Regex / Keyword Filters\n",
    "Quick and cheap—great for profanity:\n",
    "\n",
    "```python\n",
    "import re\n",
    "def rude_filter(text):\n",
    "    banned = r\"\"\"(?i)\\b(fuck|shit|damn)\\b\"\"\"\n",
    "    return bool(re.search(banned, text))\n",
    "```\n",
    "\n",
    "### 5.2 Policy Scoring (OpenAI Moderation v2)\n",
    "```python\n",
    "moderation = openai.moderations.create(input=\"Text to check\")\n",
    "if moderation.results[0].category_scores.violence > 0.5:\n",
    "    raise ValueError(\"Violent content detected\")\n",
    "```\n",
    "\n",
    "### 5.3 Safety‑Tuned Models\n",
    "Use **`gpt-4o-mini-safe`** or open‑source **`Zephyr‑Guardrails`** for safer completions out‑of‑the‑box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c46ed3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='monitor'></a>\n",
    "## 6️⃣ Monitoring & Structured Logging\n",
    "\n",
    "```python\n",
    "import logging, json, time\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "def log_event(user_prompt, model_output, latency_ms, violations):\n",
    "    record = {\n",
    "        \"ts\": time.time(),\n",
    "        \"prompt\": user_prompt[:100],\n",
    "        \"output\": model_output[:200],\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"violations\": violations\n",
    "    }\n",
    "    logging.info(json.dumps(record))\n",
    "```\n",
    "\n",
    "> Send logs to **OpenTelemetry**, **Datadog**, or **Prometheus** for dashboards and alerting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ee6e3",
   "metadata": {},
   "source": [
    "\n",
    "<a id='eval'></a>\n",
    "## 7️⃣ Offline Evaluation Harness (Quality + Guardrail Coverage)\n",
    "\n",
    "We’ll score our guardrailed pipeline on **100 conversational samples**.\n",
    "\n",
    "```python\n",
    "!pip -q install ragas==0.1.7 datasets evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "accuracy = load(\"accuracy\")\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test[:100]\")\n",
    "passes = 0\n",
    "\n",
    "for row in dataset:\n",
    "    try:\n",
    "        validated = guard.parse(chat(row[\"chosen\"].split(\"\\n\\nAssistant: \")[-1])[0])\n",
    "        passes += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Schema pass rate:\", passes/len(dataset))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057601e8",
   "metadata": {},
   "source": [
    "\n",
    "<a id='fallback'></a>\n",
    "## 8️⃣ Fallback & Graceful Degradation\n",
    "\n",
    "1. **Retry** with higher max‑tokens or lower temperature.  \n",
    "2. **Switch Model** (gpt‑4o → gpt‑3.5‑turbo → TinyLlama).  \n",
    "3. **Zero‑Shot → Few‑Shot**: add examples for tricky queries.  \n",
    "4. **Return Partial**: deliver best‑effort answer plus `\"uncertain\": true`.  \n",
    "5. **Escalate to Human** for critical violations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c214d",
   "metadata": {},
   "source": [
    "\n",
    "<a id='cost'></a>\n",
    "## 9️⃣ Cost / Latency Engineering\n",
    "\n",
    "| Lever | Effect | Trade‑off |\n",
    "|-------|--------|-----------|\n",
    "| **Context Length** | ↓ tokens → ↓ cost/latency | Risk missing info |\n",
    "| **Streaming** | Faster first token | More complex client |\n",
    "| **Caching** | 80/20 on repeated prompts | Stale answers |\n",
    "| **Batching** | Amortise API overhead | Higher p90 latency |\n",
    "| **Quantized Local Models** | Cheap inference | Lower quality |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47509da0",
   "metadata": {},
   "source": [
    "\n",
    "<a id='ex'></a>\n",
    "## 🔨 Exercises\n",
    "\n",
    "1. **Policy Stress‑Test**  \n",
    "   Create 10 prompts likely to violate safety policies. Measure violation catch‑rate with OpenAI Moderations *vs.* regex filter.\n",
    "\n",
    "2. **Build Your Own Rail**  \n",
    "   Define an XML Rail for a *travel‑itinerary* generator that outputs a list of dicts with `city`, `days`, `highlights`.\n",
    "\n",
    "3. **Latency Budget**  \n",
    "   Using `time.time`, benchmark raw model vs. guardrailed retries. Plot latency distribution.\n",
    "\n",
    "4. **AB Compare**  \n",
    "   Route 1 000 sample prompts through two guardrail configs (strict vs. relaxed) and compare schema pass‑rate and user satisfaction (simulated with sentiment).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a7c1c",
   "metadata": {},
   "source": [
    "\n",
    "<a id='read'></a>\n",
    "## 📚 Further Reading\n",
    "\n",
    "* **“Guardrails: A Framework for Verifiable and Reliable LLMs”** (arXiv 2023)  \n",
    "* OpenAI **Function Calling & JSON Schema** docs  \n",
    "* LangChain Docs — **Output Parsers & Validators**  \n",
    "* Microsoft Responsible AI Toolbox — **Text Safety**  \n",
    "* RAGAS: **Evaluation for Retrieval‑Augmented Generation**  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
