{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998db4be",
   "metadata": {},
   "source": [
    "# Lab 3 â€“ Logging, Deployment, and Automated Evaluation\n",
    "Track, scale, and evaluate promptâ€‘driven applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80005a",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Instrument logging with PromptLayer.\n",
    "2. Discuss deployment patterns.\n",
    "3. Integrate automated evals into pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install promptlayer openai langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295fe4",
   "metadata": {},
   "source": [
    "### 1.Â PromptLayer Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptlayer, os, getpass\n",
    "os.environ[\"PROMPTLAYER_API_KEY\"] = getpass.getpass(\"ðŸ”‘ PromptLayer API Key: \")\n",
    "promptlayer.apikey = os.getenv(\"PROMPTLAYER_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import PromptLayerChatOpenAI\n",
    "llm = PromptLayerChatOpenAI(model_name=\"gpt-3.5-turbo\", pl_tags=[\"day9_lab\"])\n",
    "resp = llm([HumanMessage(content=\"Hello, world!\")])\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2564f",
   "metadata": {},
   "source": [
    "### 2.Â Logging Best Practices\n",
    "- Log **inputs**, **outputs**, metadata, model/version.\n",
    "- Capture errors and latency.\n",
    "- Tag experiments for A/B tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a4678",
   "metadata": {},
   "source": [
    "### 3.Â Deployment Sketch (Serverless REST API)\n",
    "Below is a minimal FastAPI stub. Uncomment to generate `main.py` for deployment on Cloud Run / Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ee1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile main.py\n",
    "# import os\n",
    "# from fastapi import FastAPI\n",
    "# from pydantic import BaseModel\n",
    "# from langchain.chains import SimpleSequentialChain\n",
    "# app = FastAPI()\n",
    "# class Payload(BaseModel):\n",
    "#     topic: str\n",
    "# @app.post(\"/generate\")\n",
    "# def generate(p: Payload):\n",
    "#     return pipeline.invoke({\"topic\": p.topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753cb75",
   "metadata": {},
   "source": [
    "### 4.Â Automated Evaluation with OpenAI Evals (conceptual)\n",
    "OpenAI Evals requires CLI setup; here we draft a basic template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12483a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install openai-evals\n",
    "# !oaieval samples/accuracy evals/my_eval.yaml --record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2456a",
   "metadata": {},
   "source": [
    "### 5.Â Capstone Integration\n",
    "Use the following checklist to integrate logging + evaluation into your project:\n",
    "1. Instrument with PromptLayer.\n",
    "2. Add FastAPI wrapper.\n",
    "3. Write an Eval spec.\n",
    "4. Automate via CI/CD."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
