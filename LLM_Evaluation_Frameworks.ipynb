{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0619b921",
   "metadata": {},
   "source": [
    "# LLM Evaluation Frameworks  \n",
    "### TruthfulQA • LM Harness • HELM • OpenAI Evals  \n",
    "\n",
    "**Author:** _Your Name_  \n",
    "**Course:** Prompt Engineering – Evaluation Module  \n",
    "\n",
    "---\n",
    "\n",
    "This Colab notebook is a **hands‑on tour** of four major frameworks & datasets used to benchmark Large Language Models (LLMs).\n",
    "\n",
    "| Framework / Dataset | Purpose | Key Metrics | Typical Use‑cases |\n",
    "|--------------------|---------|-------------|-------------------|\n",
    "| **TruthfulQA** | Dataset to test factual correctness vs. common misconceptions | Truthful accuracy, eval harness accuracy | Research on hallucinations & truthfulness |\n",
    "| **LM Harness** | Unified wrapper around dozens of tasks / datasets | Task‑specific (accuracy, F1, BLEU, etc.) | Rapid leaderboard & model regression tests |\n",
    "| **HELM** | Holistic evaluation across scenario, bias, robustness | Multiple: exact‑match, toxicity, robustness deltas | Transparency dashboards & broad model audits |\n",
    "| **OpenAI Evals** | Flexible YAML + Python spec to score any model | User‑defined; integrates with OpenAI API | Custom evals, gated model & system tests |\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Install** each tool in a fresh Colab runtime.  \n",
    "2. **Run** a *minimal* evaluation against an open‑source or OpenAI model.  \n",
    "3. **Interpret** the metrics produced.  \n",
    "4. **Compare** trade‑offs when choosing an eval framework for class projects or production.\n",
    "\n",
    "> ⚠️ **Note**: Full evaluations can take hours and cost money (OpenAI API).  We use _tiny subsets_ to keep runtimes & costs low for learning purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c25ce",
   "metadata": {},
   "source": [
    "## ⏬ Setup — Install Dependencies\n",
    "Running the next cell installs lightweight versions of the required packages. If you’re on an Arm‑based Colab, add `--no-binary :all:` flags where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip -q install \"lm-eval==0.4.0\" \"openai-evals==0.3.0\" \"helm-benchmark==0.4.1\" --progress-bar off\n",
    "python -m pip -q install --upgrade openai\n",
    "echo '✅ Packages installed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450a2f9",
   "metadata": {},
   "source": [
    "## 🔐 Configure Credentials\n",
    "Only **OpenAI Evals** requires an API key. If you do not plan on using OpenAI models you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3461c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass, json, textwrap, warnings\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('Paste your OpenAI API key (leave blank to skip): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54854786",
   "metadata": {},
   "source": [
    "## 1️⃣ TruthfulQA via LM Harness  \n",
    "\n",
    "[TruthfulQA](https://github.com/sylinrl/TruthfulQA) challenges models on 817 questions designed to elicit **common false beliefs**.  \n",
    "We run the **multiple‑choice** variant (`truthfulqa_mc`) on a small open‑source model for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=\"pretrained=gpt2\",  # tiny model for demo\n",
    "    tasks=[\"truthfulqa_mc\"],\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# Extract a key metric\n",
    "truth_score = results[\"results\"][\"truthfulqa_mc\"][\"mc1\"]\n",
    "print(f\"GPT‑2 TruthfulQA MC accuracy: {truth_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e948ed",
   "metadata": {},
   "source": [
    "### What to look for  \n",
    "* **`mc1`**: accuracy when the highest probability *single* choice must be correct.  \n",
    "Low scores on small models are expected. State‑of‑the‑art models (2025) reach >80 %.  \n",
    "Try swapping `pretrained=gpt2` for `pretrained=meta-llama/Meta-Llama-3-8B` (requires 🤗 token)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c72d12",
   "metadata": {},
   "source": [
    "## 2️⃣ LM Harness Quick‑Compare\n",
    "The harness supports 200+ tasks. Below we sweep two small tasks to create a **mini leaderboard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f78e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"piqa\", \"hellaswag[:128]\"]  # tiny slice for demo\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=\"pretrained=sshleifer/tiny-gpt2\",\n",
    "    tasks=tasks,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({k: v for k, v in results[\"results\"].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c127e",
   "metadata": {},
   "source": [
    "Notice how **task adapters** abstract away dataset loading and metric computation. You can mix‑and‑match HuggingFace or OpenAI models via the same CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276036b6",
   "metadata": {},
   "source": [
    "## 3️⃣ HELM (Holistic Evaluation of Language Models)  \n",
    "\n",
    "HELM provides **scenario‑based** benchmarking plus a rich HTML dashboard.  \n",
    "Running the full suite is heavy (≈ 15 GB data). Instead we demonstrate a *single scenario* locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate a stub run directory\n",
    "helm-run   --suite accuracy --model gpt2 --scenario squad   --max-eval-instances 16   --output-dir helm_demo\n",
    "\n",
    "# Display summary JSON\n",
    "python - <<'PY'\n",
    "import json, pathlib, pprint, os\n",
    "summary = pathlib.Path(\"helm_demo/summary.json\")\n",
    "if summary.exists():\n",
    "    data = json.loads(summary.read_text())\n",
    "    pprint.pp({k:v for k,v in data.items() if k.startswith('metrics')})\n",
    "else:\n",
    "    print(\"HELM run may have failed. Reduce `--max-eval-instances` if RAM is low.\")\n",
    "PY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0dd91",
   "metadata": {},
   "source": [
    "**HELM key ideas**\n",
    "\n",
    "* **Scenario** = dataset + prompting strategy + metric set.  \n",
    "* **Suite** = group of scenarios (e.g., *accuracy*, *robustness*, *bias*).  \n",
    "* Generates an interactive **HTML report** (`index.html` in `helm_demo`). Download and open locally or via Colab `files.download`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33361c94",
   "metadata": {},
   "source": [
    "## 4️⃣ OpenAI Evals  \n",
    "\n",
    "OpenAI Evals lets you define tests via **YAML + Jinja** (prompt templates) or pure Python.  \n",
    "Below we clone a starter eval that checks a model’s ability to sort numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "evals init my_sort_eval --task simple_sort\n",
    "cd my_sort_eval\n",
    "echo \"✅ Eval scaffold created\"\n",
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28625441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run the eval locally (uses your OPENAI_API_KEY)\n",
    "cd my_sort_eval\n",
    "evals run my_sort_eval   --model gpt-3.5-turbo-1106   --max_samples 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09848d7",
   "metadata": {},
   "source": [
    "The output prints **pass/fail counts** and logs.  \n",
    "You can **register** your eval with `evals registry`.  \n",
    "Try editing `prompts/prompt.jinja` to make the task harder, then re‑run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953cedb",
   "metadata": {},
   "source": [
    "## 🔄 Framework Comparison Cheat‑Sheet  \n",
    "\n",
    "| Feature | LM Harness | TruthfulQA | HELM | OpenAI Evals |\n",
    "|---------|------------|------------|------|--------------|\n",
    "| **Install weight** | Light | Light (dataset only) | Heavy | Medium |\n",
    "| **Task coverage** | 200+ | 1 (truthfulness) | 40+ scenarios | Custom by user |\n",
    "| **Metrics** | Task‑specific | mc1, mc2, BLEU | Varied (accuracy → toxicity) | User‑defined |\n",
    "| **Extensible** | ✅ YAML/JSON | Limited | ✅ Python | ✅ YAML + Python |\n",
    "| **Dashboard** | CLI / JSON | CSV | HTML | JSON / OpenAI Console |\n",
    "| **Best for** | Quick model sweeps | Hallucination studies | Holistic audits | CI/CD model gating |\n",
    "\n",
    "**Take‑aways**\n",
    "\n",
    "* Use **LM Harness** for *breadth* and leaderboard style comparison.  \n",
    "* Add **TruthfulQA** when measuring *truthfulness/hallucination*.  \n",
    "* Run **HELM** for research‑grade, multi‑axis audits (bias, robustness).  \n",
    "* Embed **OpenAI Evals** into pipelines for regression testing and custom data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680f61d",
   "metadata": {},
   "source": [
    "## 📝 Hands‑On Exercises  \n",
    "\n",
    "1. **Swap models** – Replace `gpt2` with a newer model in Sections 1–2 and record the score delta.  \n",
    "2. **Create a custom LM Harness task** – Follow the docs to wrap a JSONL dataset of your choice.  \n",
    "3. **Add a bias scenario to HELM** – Adjust the `--suite` flag. Which metrics appear?  \n",
    "4. **Write a new OpenAI Eval** – Build a math‑word‑problem checker and run it on `gpt‑4o`.  \n",
    "\n",
    "Try at least one exercise before the next class and share screenshots of your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64263d",
   "metadata": {},
   "source": [
    "## 📚 Further Reading & Resources  \n",
    "\n",
    "* TruthfulQA paper & dataset – Lin et al., 2022.  \n",
    "* LM Evaluation Harness docs (GitHub).  \n",
    "* HELM whitepaper – Stanford CRFM, 2022 → 2025 updates.  \n",
    "* OpenAI Evals Cookbook example (`cookbook.openai.com`).  \n",
    "\n",
    "---\n",
    "\n",
    "© 2025 Prompt Engineering Course – Licensed CC‑BY‑SA  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
