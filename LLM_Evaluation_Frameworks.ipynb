{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0619b921",
   "metadata": {},
   "source": [
    "# LLM Evaluation Frameworks  \n",
    "### TruthfulQA â€¢ LM Harness â€¢ HELM â€¢ OpenAIÂ Evals  \n",
    "\n",
    "**Author:** _YourÂ Name_  \n",
    "**Course:** Prompt Engineering â€“ Evaluation Module  \n",
    "\n",
    "---\n",
    "\n",
    "This Colab notebook is a **handsâ€‘on tour** of four major frameworks & datasets used to benchmark Large LanguageÂ Models (LLMs).\n",
    "\n",
    "| Framework / Dataset | Purpose | Key Metrics | Typical Useâ€‘cases |\n",
    "|--------------------|---------|-------------|-------------------|\n",
    "| **TruthfulQA** | Dataset to test factual correctness vs. common misconceptions | TruthfulÂ accuracy, eval harness accuracy | Research on hallucinations & truthfulness |\n",
    "| **LM Harness** | Unified wrapper around dozens of tasks / datasets | Taskâ€‘specific (accuracy, F1, BLEU, etc.) | Rapid leaderboard & model regression tests |\n",
    "| **HELM** | Holistic evaluation across scenario, bias, robustness | Multiple: exactâ€‘match, toxicity, robustness deltas | Transparency dashboards & broad model audits |\n",
    "| **OpenAI Evals** | Flexible YAML + Python spec to score any model | Userâ€‘defined; integrates with OpenAIÂ API | Custom evals, gated model & system tests |\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "1. **Install** each tool in a fresh Colab runtime.  \n",
    "2. **Run** a *minimal* evaluation against an openâ€‘source or OpenAI model.  \n",
    "3. **Interpret** the metrics produced.  \n",
    "4. **Compare** tradeâ€‘offs when choosing an eval framework for class projects or production.\n",
    "\n",
    "> âš ï¸ **Note**: Full evaluations can take hours and cost money (OpenAIâ€¯API).  We use _tiny subsets_ to keep runtimes & costs low for learning purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c25ce",
   "metadata": {},
   "source": [
    "## â¬Â Setup â€” Install Dependencies\n",
    "Running the next cell installs lightweight versions of the required packages.Â If youâ€™re on an Armâ€‘based Colab, add `--no-binary :all:` flags where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip -q install \"lm-eval==0.4.0\" \"openai-evals==0.3.0\" \"helm-benchmark==0.4.1\" --progress-bar off\n",
    "python -m pip -q install --upgrade openai\n",
    "echo 'âœ… Packages installed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450a2f9",
   "metadata": {},
   "source": [
    "## ğŸ”Â Configure Credentials\n",
    "Only **OpenAIÂ Evals** requires an API key.Â If you do not plan on using OpenAI models you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3461c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass, json, textwrap, warnings\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('Paste your OpenAI API key (leave blank to skip): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54854786",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£Â TruthfulQA via LM Harness  \n",
    "\n",
    "[TruthfulQA](https://github.com/sylinrl/TruthfulQA) challenges models on 817 questions designed to elicit **common false beliefs**.  \n",
    "We run the **multipleâ€‘choice** variant (`truthfulqa_mc`) on a small openâ€‘source model for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import evaluator\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=\"pretrained=gpt2\",  # tiny model for demo\n",
    "    tasks=[\"truthfulqa_mc\"],\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# Extract a key metric\n",
    "truth_score = results[\"results\"][\"truthfulqa_mc\"][\"mc1\"]\n",
    "print(f\"GPTâ€‘2 TruthfulQAÂ MC accuracy: {truth_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e948ed",
   "metadata": {},
   "source": [
    "### What to look for  \n",
    "* **`mc1`**: accuracy when the highest probability *single* choice must be correct.  \n",
    "Low scores on small models are expected.Â Stateâ€‘ofâ€‘theâ€‘art models (2025) reach >80â€¯%.  \n",
    "Try swapping `pretrained=gpt2` for `pretrained=meta-llama/Meta-Llama-3-8B` (requires ğŸ¤— token)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c72d12",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£Â LM Harness Quickâ€‘Compare\n",
    "The harness supports 200+ tasks.Â Below we sweep two small tasks to create a **mini leaderboard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f78e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"piqa\", \"hellaswag[:128]\"]  # tiny slice for demo\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=\"pretrained=sshleifer/tiny-gpt2\",\n",
    "    tasks=tasks,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({k: v for k, v in results[\"results\"].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c127e",
   "metadata": {},
   "source": [
    "Notice how **task adapters** abstract away dataset loading and metric computation.Â You can mixâ€‘andâ€‘match HuggingFace or OpenAI models via the same CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276036b6",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£Â HELM (HolisticÂ Evaluation of Language Models)  \n",
    "\n",
    "HELM provides **scenarioâ€‘based** benchmarking plus a rich HTML dashboard.  \n",
    "Running the full suite is heavy (â‰ˆâ€¯15â€¯GB data).Â Instead we demonstrate a *single scenario* locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate a stub run directory\n",
    "helm-run   --suite accuracy --model gpt2 --scenario squad   --max-eval-instances 16   --output-dir helm_demo\n",
    "\n",
    "# Display summary JSON\n",
    "python - <<'PY'\n",
    "import json, pathlib, pprint, os\n",
    "summary = pathlib.Path(\"helm_demo/summary.json\")\n",
    "if summary.exists():\n",
    "    data = json.loads(summary.read_text())\n",
    "    pprint.pp({k:v for k,v in data.items() if k.startswith('metrics')})\n",
    "else:\n",
    "    print(\"HELM run may have failed. Reduce `--max-eval-instances` if RAM is low.\")\n",
    "PY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0dd91",
   "metadata": {},
   "source": [
    "**HELM key ideas**\n",
    "\n",
    "* **Scenario** = dataset + prompting strategy + metric set.  \n",
    "* **Suite** = group of scenarios (e.g., *accuracy*, *robustness*, *bias*).  \n",
    "* Generates an interactive **HTML report** (`index.html` in `helm_demo`). Download and open locally or via Colab `files.download`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33361c94",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£Â OpenAIÂ Evals  \n",
    "\n",
    "OpenAI Evals lets you define tests via **YAML + Jinja** (prompt templates) or pure Python.  \n",
    "Below we clone a starter eval that checks a modelâ€™s ability to sort numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "evals init my_sort_eval --task simple_sort\n",
    "cd my_sort_eval\n",
    "echo \"âœ…Â Eval scaffold created\"\n",
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28625441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run the eval locally (uses your OPENAI_API_KEY)\n",
    "cd my_sort_eval\n",
    "evals run my_sort_eval   --model gpt-3.5-turbo-1106   --max_samples 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09848d7",
   "metadata": {},
   "source": [
    "The output prints **pass/fail counts** and logs.  \n",
    "You can **register** your eval with `evals registry`.  \n",
    "Try editing `prompts/prompt.jinja` to make the task harder, then reâ€‘run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953cedb",
   "metadata": {},
   "source": [
    "## ğŸ”„Â Framework Comparison Cheatâ€‘Sheet  \n",
    "\n",
    "| Feature | LM Harness | TruthfulQA | HELM | OpenAIÂ Evals |\n",
    "|---------|------------|------------|------|--------------|\n",
    "| **Install weight** | Light | Light (dataset only) | Heavy | Medium |\n",
    "| **Task coverage** | 200+ | 1 (truthfulness) | 40+ scenarios | Custom by user |\n",
    "| **Metrics** | Taskâ€‘specific | mc1, mc2, BLEU | Varied (accuracyÂ â†’Â toxicity) | Userâ€‘defined |\n",
    "| **Extensible** | âœ… YAML/JSON | Limited | âœ… Python | âœ… YAMLÂ +Â Python |\n",
    "| **Dashboard** | CLI / JSON | CSV | HTML | JSON / OpenAI Console |\n",
    "| **Best for** | Quick model sweeps | Hallucination studies | Holistic audits | CI/CD model gating |\n",
    "\n",
    "**Takeâ€‘aways**\n",
    "\n",
    "* Use **LM Harness** for *breadth* and leaderboard style comparison.  \n",
    "* Add **TruthfulQA** when measuring *truthfulness/hallucination*.  \n",
    "* Run **HELM** for researchâ€‘grade, multiâ€‘axis audits (bias, robustness).  \n",
    "* Embed **OpenAIÂ Evals** into pipelines for regression testing and custom data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680f61d",
   "metadata": {},
   "source": [
    "## ğŸ“Â Handsâ€‘On Exercises  \n",
    "\n",
    "1. **Swap models** â€“ Replace `gpt2` with a newer model in SectionsÂ 1â€“2 and record the score delta.  \n",
    "2. **Create a custom LMÂ Harness task** â€“ Follow the docs to wrap a JSONL dataset of your choice.  \n",
    "3. **Add a bias scenario to HELM** â€“ Adjust the `--suite` flag. Which metrics appear?  \n",
    "4. **Write a new OpenAI Eval** â€“ Build a mathâ€‘wordâ€‘problem checker and run it on `gptâ€‘4o`.  \n",
    "\n",
    "Try at least one exercise before the next class and share screenshots of your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64263d",
   "metadata": {},
   "source": [
    "## ğŸ“šÂ Further Reading & Resources  \n",
    "\n",
    "* TruthfulQA paper & dataset â€“ LinÂ etÂ al., 2022.  \n",
    "* LM Evaluation Harness docs (GitHub).  \n",
    "* HELM whitepaper â€“ Stanford CRFM, 2022â€¯â†’â€¯2025 updates.  \n",
    "* OpenAIÂ Evals Cookbook example (`cookbook.openai.com`).  \n",
    "\n",
    "---\n",
    "\n",
    "Â© 2025 Prompt Engineering Course â€“ Licensed CCâ€‘BYâ€‘SA  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
